{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Inferneo","text":"<p>Lightning-fast, scalable inference server for modern AI models</p> <p> </p> <p>Inferneo is a high-performance inference server designed for serving large language models and other AI models with exceptional speed and efficiency.</p>"},{"location":"#why-choose-inferneo","title":"\ud83d\ude80 Why Choose Inferneo?","text":""},{"location":"#blazing-fast-performance","title":"Blazing Fast Performance","text":"<ul> <li>State-of-the-art throughput with optimized CUDA kernels</li> <li>Efficient memory management with advanced attention mechanisms</li> <li>Continuous batching for maximum GPU utilization</li> <li>Speculative decoding for faster text generation</li> <li>Chunked prefill for improved latency</li> </ul>"},{"location":"#production-ready","title":"Production Ready","text":"<ul> <li>Horizontal scaling with distributed inference</li> <li>Load balancing and automatic failover</li> <li>Real-time monitoring and metrics</li> <li>Health checks and graceful degradation</li> <li>Multi-tenant support with resource isolation</li> </ul>"},{"location":"#developer-friendly","title":"Developer Friendly","text":"<ul> <li>Simple Python API for easy integration</li> <li>REST API compatible with OpenAI standards</li> <li>WebSocket support for streaming responses</li> <li>Docker containers for easy deployment</li> <li>Kubernetes operators for cloud-native deployment</li> </ul>"},{"location":"#key-features","title":"\ud83c\udfaf Key Features","text":""},{"location":"#model-support","title":"Model Support","text":"<ul> <li>Hugging Face Transformers - Seamless integration</li> <li>Custom model formats - ONNX, TorchScript, TensorRT</li> <li>Multi-modal models - Text, vision, audio</li> <li>Quantized models - INT4, INT8, FP16 support</li> <li>LoRA adapters - Dynamic model switching</li> </ul>"},{"location":"#advanced-optimizations","title":"Advanced Optimizations","text":"<ul> <li>Dynamic batching - Automatic request grouping</li> <li>Memory pooling - Efficient GPU memory usage</li> <li>Kernel fusion - Optimized CUDA operations</li> <li>Attention optimization - FlashAttention integration</li> <li>Pipeline parallelism - Multi-GPU scaling</li> </ul>"},{"location":"#enterprise-features","title":"Enterprise Features","text":"<ul> <li>Authentication &amp; Authorization - JWT, OAuth2, API keys</li> <li>Rate limiting - Per-user and per-model quotas</li> <li>Request logging - Comprehensive audit trails</li> <li>Model versioning - A/B testing and rollbacks</li> <li>Cost optimization - Resource usage analytics</li> </ul>"},{"location":"#performance-benchmarks","title":"\ud83d\udcca Performance Benchmarks","text":"Model Batch Size Throughput Latency (p50) GPU Memory Llama-2-7B 32 1,200 tokens/s 45ms 14GB Llama-2-13B 16 850 tokens/s 65ms 28GB Llama-2-70B 4 320 tokens/s 180ms 80GB <p>Benchmarks on NVIDIA A100-80GB with continuous batching enabled</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Install Inferneo\npip install inferneo\n\n# Start the server\ninferneo serve --model meta-llama/Llama-2-7b-chat-hf\n\n# Make a request\ncurl -X POST \"http://localhost:8000/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre>"},{"location":"#documentation-sections","title":"\ud83d\udcda Documentation Sections","text":"<ul> <li>Getting Started - Quick setup guide</li> <li>Installation - Detailed installation instructions</li> <li>User Guide - Comprehensive usage documentation</li> <li>Examples - Code examples and tutorials</li> <li>API Reference - Complete API documentation</li> <li>CLI Reference - Command-line interface docs</li> <li>Developer Guide - Contributing and development</li> <li>Community - Roadmap, releases, and FAQ</li> </ul>"},{"location":"#community-support","title":"\ud83e\udd1d Community &amp; Support","text":"<ul> <li>GitHub: inferneo/inferneo</li> <li>Discord: Join our community</li> <li>Twitter: @inferneo_ai</li> <li>Blog: inferneo.ai/blog</li> </ul>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>Inferneo is licensed under the Apache 2.0 License. See the LICENSE file for details.</p> <p>Inferneo is designed and built by the AI community, for the AI community. </p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#overview","title":"Overview","text":"<p>Quick setup guide for Inferneo.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>CUDA-compatible GPU (optional)</li> <li>Docker (optional)</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#using-pip","title":"Using pip","text":"<pre><code>pip install inferneo\n</code></pre>"},{"location":"getting-started/#using-docker","title":"Using Docker","text":"<pre><code>docker pull inferneo/inferneo\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#start-the-server","title":"Start the server","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"getting-started/#make-your-first-request","title":"Make your first request","text":"<pre><code>import requests\n\nresponse = requests.post(\"http://localhost:8000/v1/chat/completions\", \n    json={\n        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }\n)\nprint(response.json())\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore batching: Batching Guide</li> <li>Set up distributed inference: Distributed Inference</li> <li>Deploy to production: Production deployment guide </li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#overview","title":"Overview","text":"<p>Installation guide for Inferneo.</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.8+</li> <li>CUDA 11.8+ (for GPU support)</li> <li>16GB+ RAM</li> <li>NVIDIA GPU (recommended)</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#using-pip","title":"Using pip","text":"<pre><code>pip install inferneo\n</code></pre>"},{"location":"installation/#using-conda","title":"Using conda","text":"<pre><code>conda install -c conda-forge inferneo\n</code></pre>"},{"location":"installation/#using-docker","title":"Using Docker","text":"<pre><code>docker pull inferneo/inferneo\n</code></pre>"},{"location":"installation/#gpu-support","title":"GPU Support","text":""},{"location":"installation/#cuda-installation","title":"CUDA Installation","text":"<p>Install CUDA toolkit for GPU acceleration.</p>"},{"location":"installation/#pytorch-with-cuda","title":"PyTorch with CUDA","text":"<pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>python -c \"import inferneo; print('Inferneo installed successfully!')\"\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started</li> <li>User Guide</li> <li>Examples </li> </ul>"},{"location":"api-reference/configuration/","title":"Configuration","text":"<p>Inferneo can be configured through various methods including configuration files, environment variables, and command-line arguments.</p>"},{"location":"api-reference/configuration/#configuration-file","title":"Configuration File","text":""},{"location":"api-reference/configuration/#yaml-configuration","title":"YAML Configuration","text":"<p>Create a <code>config.yaml</code> file in your project directory:</p> <pre><code># Server configuration\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n  cors_origins: [\"*\"]\n  max_request_size: \"10MB\"\n  timeout: 300\n\n# Model configuration\nmodel:\n  name: \"meta-llama/Llama-2-7b-chat-hf\"\n  tensor_parallel_size: 1\n  pipeline_parallel_size: 1\n  max_model_len: 4096\n  gpu_memory_utilization: 0.8\n  swap_space: 4\n  quantization: \"awq\"\n  quantization_config:\n    bits: 4\n    group_size: 128\n    zero_point: true\n    scale: true\n\n# Inference configuration\ninference:\n  max_batch_size: 32\n  max_batch_tokens: 4096\n  max_num_seqs: 256\n  max_num_batched_tokens: 4096\n  max_paddings: 256\n  max_num_seqs_per_prompt: 1\n  max_num_batched_tokens_per_prompt: 2048\n  max_num_seqs_per_batch: 32\n  max_num_batched_tokens_per_batch: 4096\n\n# Performance configuration\nperformance:\n  max_concurrent_requests: 100\n  max_concurrent_streaming_requests: 50\n  max_concurrent_batched_requests: 10\n  max_concurrent_batched_streaming_requests: 5\n  max_concurrent_batched_tokens: 4096\n  max_concurrent_batched_streaming_tokens: 2048\n\n# Logging configuration\nlogging:\n  level: \"INFO\"\n  format: \"json\"\n  file: \"inferneo.log\"\n  max_size: \"100MB\"\n  backup_count: 5\n\n# Security configuration\nsecurity:\n  api_key: \"your-api-key\"\n  allowed_origins: [\"*\"]\n  rate_limit:\n    requests_per_minute: 60\n    tokens_per_minute: 150000\n    requests_per_day: 3500\n\n# Monitoring configuration\nmonitoring:\n  enabled: true\n  metrics_port: 8001\n  health_check_interval: 30\n  prometheus_enabled: true\n</code></pre>"},{"location":"api-reference/configuration/#json-configuration","title":"JSON Configuration","text":"<p>Alternatively, use JSON format:</p> <pre><code>{\n  \"server\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 8000,\n    \"cors_origins\": [\"*\"],\n    \"max_request_size\": \"10MB\",\n    \"timeout\": 300\n  },\n  \"model\": {\n    \"name\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"tensor_parallel_size\": 1,\n    \"pipeline_parallel_size\": 1,\n    \"max_model_len\": 4096,\n    \"gpu_memory_utilization\": 0.8,\n    \"swap_space\": 4,\n    \"quantization\": \"awq\",\n    \"quantization_config\": {\n      \"bits\": 4,\n      \"group_size\": 128,\n      \"zero_point\": true,\n      \"scale\": true\n    }\n  },\n  \"inference\": {\n    \"max_batch_size\": 32,\n    \"max_batch_tokens\": 4096,\n    \"max_num_seqs\": 256,\n    \"max_num_batched_tokens\": 4096,\n    \"max_paddings\": 256\n  },\n  \"performance\": {\n    \"max_concurrent_requests\": 100,\n    \"max_concurrent_streaming_requests\": 50,\n    \"max_concurrent_batched_requests\": 10\n  },\n  \"logging\": {\n    \"level\": \"INFO\",\n    \"format\": \"json\",\n    \"file\": \"inferneo.log\"\n  },\n  \"security\": {\n    \"api_key\": \"your-api-key\",\n    \"allowed_origins\": [\"*\"],\n    \"rate_limit\": {\n      \"requests_per_minute\": 60,\n      \"tokens_per_minute\": 150000,\n      \"requests_per_day\": 3500\n    }\n  },\n  \"monitoring\": {\n    \"enabled\": true,\n    \"metrics_port\": 8001,\n    \"health_check_interval\": 30,\n    \"prometheus_enabled\": true\n  }\n}\n</code></pre>"},{"location":"api-reference/configuration/#environment-variables","title":"Environment Variables","text":"<p>Configure Inferneo using environment variables:</p> <pre><code># Server configuration\nexport INFERNEO_HOST=\"0.0.0.0\"\nexport INFERNEO_PORT=8000\nexport INFERNEO_CORS_ORIGINS=\"*\"\nexport INFERNEO_MAX_REQUEST_SIZE=\"10MB\"\nexport INFERNEO_TIMEOUT=300\n\n# Model configuration\nexport INFERNEO_MODEL_NAME=\"meta-llama/Llama-2-7b-chat-hf\"\nexport INFERNEO_TENSOR_PARALLEL_SIZE=1\nexport INFERNEO_PIPELINE_PARALLEL_SIZE=1\nexport INFERNEO_MAX_MODEL_LEN=4096\nexport INFERNEO_GPU_MEMORY_UTILIZATION=0.8\nexport INFERNEO_SWAP_SPACE=4\nexport INFERNEO_QUANTIZATION=\"awq\"\nexport INFERNEO_QUANTIZATION_BITS=4\nexport INFERNEO_QUANTIZATION_GROUP_SIZE=128\n\n# Inference configuration\nexport INFERNEO_MAX_BATCH_SIZE=32\nexport INFERNEO_MAX_BATCH_TOKENS=4096\nexport INFERNEO_MAX_NUM_SEQS=256\nexport INFERNEO_MAX_NUM_BATCHED_TOKENS=4096\n\n# Performance configuration\nexport INFERNEO_MAX_CONCURRENT_REQUESTS=100\nexport INFERNEO_MAX_CONCURRENT_STREAMING_REQUESTS=50\nexport INFERNEO_MAX_CONCURRENT_BATCHED_REQUESTS=10\n\n# Logging configuration\nexport INFERNEO_LOG_LEVEL=\"INFO\"\nexport INFERNEO_LOG_FORMAT=\"json\"\nexport INFERNEO_LOG_FILE=\"inferneo.log\"\n\n# Security configuration\nexport INFERNEO_API_KEY=\"your-api-key\"\nexport INFERNEO_ALLOWED_ORIGINS=\"*\"\nexport INFERNEO_RATE_LIMIT_REQUESTS_PER_MINUTE=60\nexport INFERNEO_RATE_LIMIT_TOKENS_PER_MINUTE=150000\nexport INFERNEO_RATE_LIMIT_REQUESTS_PER_DAY=3500\n\n# Monitoring configuration\nexport INFERNEO_MONITORING_ENABLED=true\nexport INFERNEO_METRICS_PORT=8001\nexport INFERNEO_HEALTH_CHECK_INTERVAL=30\nexport INFERNEO_PROMETHEUS_ENABLED=true\n</code></pre>"},{"location":"api-reference/configuration/#command-line-arguments","title":"Command-Line Arguments","text":"<p>Start Inferneo with command-line arguments:</p> <pre><code># Basic server start\ninferneo serve --model meta-llama/Llama-2-7b-chat-hf\n\n# With configuration file\ninferneo serve --config config.yaml\n\n# With command-line arguments\ninferneo serve \\\n  --model meta-llama/Llama-2-7b-chat-hf \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --tensor-parallel-size 1 \\\n  --max-model-len 4096 \\\n  --gpu-memory-utilization 0.8 \\\n  --quantization awq \\\n  --max-batch-size 32 \\\n  --max-concurrent-requests 100\n\n# With environment variables\nINFERNEO_MODEL_NAME=\"meta-llama/Llama-2-7b-chat-hf\" \\\nINFERNEO_TENSOR_PARALLEL_SIZE=1 \\\nINFERNEO_MAX_MODEL_LEN=4096 \\\ninferneo serve\n</code></pre>"},{"location":"api-reference/configuration/#configuration-options","title":"Configuration Options","text":""},{"location":"api-reference/configuration/#server-configuration","title":"Server Configuration","text":"Option Type Default Description <code>host</code> string \"0.0.0.0\" Server host address <code>port</code> integer 8000 Server port <code>cors_origins</code> array [\"*\"] Allowed CORS origins <code>max_request_size</code> string \"10MB\" Maximum request size <code>timeout</code> integer 300 Request timeout in seconds"},{"location":"api-reference/configuration/#model-configuration","title":"Model Configuration","text":"Option Type Default Description <code>name</code> string required Model name or path <code>tensor_parallel_size</code> integer 1 Number of GPUs for tensor parallelism <code>pipeline_parallel_size</code> integer 1 Number of pipeline stages <code>max_model_len</code> integer 4096 Maximum sequence length <code>gpu_memory_utilization</code> float 0.8 GPU memory utilization ratio <code>swap_space</code> integer 4 Swap space in GB <code>quantization</code> string null Quantization method (awq, gptq, squeezellm) <code>quantization_config</code> object {} Quantization parameters"},{"location":"api-reference/configuration/#inference-configuration","title":"Inference Configuration","text":"Option Type Default Description <code>max_batch_size</code> integer 32 Maximum batch size <code>max_batch_tokens</code> integer 4096 Maximum tokens per batch <code>max_num_seqs</code> integer 256 Maximum number of sequences <code>max_num_batched_tokens</code> integer 4096 Maximum batched tokens <code>max_paddings</code> integer 256 Maximum padding tokens"},{"location":"api-reference/configuration/#performance-configuration","title":"Performance Configuration","text":"Option Type Default Description <code>max_concurrent_requests</code> integer 100 Maximum concurrent requests <code>max_concurrent_streaming_requests</code> integer 50 Maximum streaming requests <code>max_concurrent_batched_requests</code> integer 10 Maximum batched requests <code>max_concurrent_batched_streaming_requests</code> integer 5 Maximum batched streaming requests"},{"location":"api-reference/configuration/#logging-configuration","title":"Logging Configuration","text":"Option Type Default Description <code>level</code> string \"INFO\" Log level (DEBUG, INFO, WARNING, ERROR) <code>format</code> string \"json\" Log format (json, text) <code>file</code> string null Log file path <code>max_size</code> string \"100MB\" Maximum log file size <code>backup_count</code> integer 5 Number of backup files"},{"location":"api-reference/configuration/#security-configuration","title":"Security Configuration","text":"Option Type Default Description <code>api_key</code> string null API key for authentication <code>allowed_origins</code> array [\"*\"] Allowed origins for CORS <code>rate_limit</code> object {} Rate limiting configuration"},{"location":"api-reference/configuration/#monitoring-configuration","title":"Monitoring Configuration","text":"Option Type Default Description <code>enabled</code> boolean true Enable monitoring <code>metrics_port</code> integer 8001 Metrics server port <code>health_check_interval</code> integer 30 Health check interval in seconds <code>prometheus_enabled</code> boolean true Enable Prometheus metrics"},{"location":"api-reference/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"api-reference/configuration/#schema-validation","title":"Schema Validation","text":"<p>Inferneo validates configuration using JSON Schema:</p> <pre><code>from inferneo import validate_config\n\n# Validate configuration\nconfig = {\n    \"model\": {\n        \"name\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"tensor_parallel_size\": 1\n    }\n}\n\ntry:\n    validate_config(config)\n    print(\"Configuration is valid\")\nexcept Exception as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"api-reference/configuration/#configuration-testing","title":"Configuration Testing","text":"<p>Test your configuration before deployment:</p> <pre><code># Validate configuration file\ninferneo validate-config --config config.yaml\n\n# Test configuration with dry run\ninferneo serve --config config.yaml --dry-run\n\n# Check configuration and exit\ninferneo serve --config config.yaml --check-config\n</code></pre>"},{"location":"api-reference/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"api-reference/configuration/#development-configuration","title":"Development Configuration","text":"<pre><code># config-dev.yaml\nserver:\n  host: \"localhost\"\n  port: 8000\n  cors_origins: [\"http://localhost:3000\"]\n\nmodel:\n  name: \"meta-llama/Llama-2-7b-chat-hf\"\n  tensor_parallel_size: 1\n  max_model_len: 2048\n  gpu_memory_utilization: 0.7\n\ninference:\n  max_batch_size: 8\n  max_batch_tokens: 2048\n\nlogging:\n  level: \"DEBUG\"\n  format: \"text\"\n\nsecurity:\n  api_key: \"dev-api-key\"\n</code></pre>"},{"location":"api-reference/configuration/#production-configuration","title":"Production Configuration","text":"<pre><code># config-prod.yaml\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n  cors_origins: [\"https://yourdomain.com\"]\n  max_request_size: \"50MB\"\n  timeout: 600\n\nmodel:\n  name: \"meta-llama/Llama-2-70b-chat-hf\"\n  tensor_parallel_size: 4\n  pipeline_parallel_size: 2\n  max_model_len: 8192\n  gpu_memory_utilization: 0.9\n  quantization: \"awq\"\n  quantization_config:\n    bits: 4\n    group_size: 128\n\ninference:\n  max_batch_size: 64\n  max_batch_tokens: 8192\n  max_num_seqs: 512\n\nperformance:\n  max_concurrent_requests: 200\n  max_concurrent_streaming_requests: 100\n\nlogging:\n  level: \"INFO\"\n  format: \"json\"\n  file: \"/var/log/inferneo.log\"\n\nsecurity:\n  api_key: \"${INFERNEO_API_KEY}\"\n  rate_limit:\n    requests_per_minute: 120\n    tokens_per_minute: 300000\n\nmonitoring:\n  enabled: true\n  metrics_port: 8001\n  prometheus_enabled: true\n</code></pre>"},{"location":"api-reference/configuration/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":"<pre><code># config-multi-gpu.yaml\nmodel:\n  name: \"meta-llama/Llama-2-70b-chat-hf\"\n  tensor_parallel_size: 4\n  pipeline_parallel_size: 2\n  max_model_len: 8192\n  gpu_memory_utilization: 0.85\n\ninference:\n  max_batch_size: 128\n  max_batch_tokens: 16384\n\nperformance:\n  max_concurrent_requests: 500\n  max_concurrent_streaming_requests: 200\n  max_concurrent_batched_requests: 50\n\nmonitoring:\n  enabled: true\n  metrics_port: 8001\n  health_check_interval: 15\n</code></pre>"},{"location":"api-reference/configuration/#memory-constrained-configuration","title":"Memory-Constrained Configuration","text":"<pre><code># config-memory-constrained.yaml\nmodel:\n  name: \"meta-llama/Llama-2-7b-chat-hf\"\n  tensor_parallel_size: 1\n  max_model_len: 1024\n  gpu_memory_utilization: 0.6\n  quantization: \"awq\"\n  quantization_config:\n    bits: 4\n    group_size: 64\n\ninference:\n  max_batch_size: 4\n  max_batch_tokens: 1024\n  max_num_seqs: 64\n\nperformance:\n  max_concurrent_requests: 20\n  max_concurrent_streaming_requests: 10\n</code></pre>"},{"location":"api-reference/configuration/#configuration-management","title":"Configuration Management","text":""},{"location":"api-reference/configuration/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code>import os\nfrom pathlib import Path\n\ndef load_config():\n    env = os.getenv(\"INFERNEO_ENV\", \"development\")\n    config_path = Path(f\"config-{env}.yaml\")\n\n    if config_path.exists():\n        return load_yaml_config(config_path)\n    else:\n        return load_default_config()\n\n# Usage\nconfig = load_config()\n</code></pre>"},{"location":"api-reference/configuration/#configuration-inheritance","title":"Configuration Inheritance","text":"<pre><code># base.yaml\nserver:\n  host: \"0.0.0.0\"\n  port: 8000\n\nmodel:\n  tensor_parallel_size: 1\n  max_model_len: 4096\n\n# development.yaml\nextends: base.yaml\n\nserver:\n  host: \"localhost\"\n\nmodel:\n  name: \"meta-llama/Llama-2-7b-chat-hf\"\n  max_model_len: 2048\n\n# production.yaml\nextends: base.yaml\n\nmodel:\n  name: \"meta-llama/Llama-2-70b-chat-hf\"\n  tensor_parallel_size: 4\n  max_model_len: 8192\n</code></pre>"},{"location":"api-reference/configuration/#dynamic-configuration","title":"Dynamic Configuration","text":"<pre><code>from inferneo import InferneoClient\n\n# Update configuration at runtime\nclient = InferneoClient(\"http://localhost:8000\")\n\n# Update model configuration\nclient.update_config({\n    \"max_model_len\": 8192,\n    \"gpu_memory_utilization\": 0.9\n})\n\n# Update inference configuration\nclient.update_inference_config({\n    \"max_batch_size\": 64,\n    \"max_batch_tokens\": 8192\n})\n</code></pre>"},{"location":"api-reference/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api-reference/configuration/#common-configuration-issues","title":"Common Configuration Issues","text":"<p>Memory Issues <pre><code># Reduce memory usage\nmodel:\n  gpu_memory_utilization: 0.6\n  max_model_len: 1024\n  quantization: \"awq\"\n\ninference:\n  max_batch_size: 4\n  max_batch_tokens: 1024\n</code></pre></p> <p>Performance Issues <pre><code># Optimize for performance\nmodel:\n  tensor_parallel_size: 2\n  gpu_memory_utilization: 0.9\n\ninference:\n  max_batch_size: 64\n  max_batch_tokens: 8192\n\nperformance:\n  max_concurrent_requests: 200\n</code></pre></p> <p>Network Issues <pre><code># Configure for network stability\nserver:\n  timeout: 600\n  max_request_size: \"50MB\"\n\nperformance:\n  max_concurrent_requests: 50\n  max_concurrent_streaming_requests: 25\n</code></pre></p> <p>For more information about server configuration, see the server configuration guide. </p>"},{"location":"api-reference/python-client/","title":"Python Client API Reference","text":"<p>This page documents the Python client for Inferneo.</p>"},{"location":"api-reference/python-client/#installation","title":"Installation","text":"<pre><code>pip install inferneo\n</code></pre>"},{"location":"api-reference/python-client/#initialization","title":"Initialization","text":"<pre><code>from inferneo import InferneoClient\nclient = InferneoClient(\"http://localhost:8000\")\n</code></pre>"},{"location":"api-reference/python-client/#methods","title":"Methods","text":""},{"location":"api-reference/python-client/#completions","title":"Completions","text":"<pre><code>response = client.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    prompt=\"Explain AI\",\n    max_tokens=100\n)\nprint(response.choices[0].text)\n</code></pre>"},{"location":"api-reference/python-client/#chat-completions","title":"Chat Completions","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n]\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    messages=messages,\n    max_tokens=150\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"api-reference/python-client/#embeddings","title":"Embeddings","text":"<pre><code>response = client.embeddings.create(\n    model=\"meta-llama/Llama-2-7b-embeddings\",\n    input=[\"What is AI?\", \"Explain deep learning.\"]\n)\nprint(response.data[0][\"embedding\"])\n</code></pre>"},{"location":"api-reference/python-client/#vision","title":"Vision","text":"<pre><code>with open(\"cat.jpg\", \"rb\") as f:\n    image_bytes = f.read()\nresponse = client.vision.create(\n    model=\"openai/clip-vit-base-patch16\",\n    image=image_bytes\n)\nprint(response.choices[0].text)\n</code></pre>"},{"location":"api-reference/python-client/#multimodal","title":"Multimodal","text":"<pre><code>with open(\"dog.jpg\", \"rb\") as f:\n    image_bytes = f.read()\nresponse = client.multimodal.create(\n    model=\"openai/blip-2\",\n    prompt=\"Describe the image.\",\n    image=image_bytes\n)\nprint(response.choices[0].text)\n</code></pre>"},{"location":"api-reference/python-client/#list-models","title":"List Models","text":"<pre><code>models = client.models.list()\nprint([model.id for model in models.data])\n</code></pre>"},{"location":"api-reference/python-client/#error-handling","title":"Error Handling","text":"<ul> <li>All methods raise exceptions on network or API errors.</li> </ul>"},{"location":"api-reference/python-client/#next-steps","title":"Next Steps","text":"<ul> <li>REST API</li> <li>WebSocket API </li> </ul>"},{"location":"api-reference/rest-api/","title":"REST API","text":"<p>Inferneo provides a comprehensive REST API for text generation, chat completions, and model management.</p>"},{"location":"api-reference/rest-api/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000/v1\n</code></pre>"},{"location":"api-reference/rest-api/#authentication","title":"Authentication","text":"<p>Most endpoints require API key authentication:</p> <pre><code>curl -H \"Authorization: Bearer your-api-key\" \\\n     http://localhost:8000/v1/models\n</code></pre>"},{"location":"api-reference/rest-api/#models","title":"Models","text":""},{"location":"api-reference/rest-api/#list-models","title":"List Models","text":"<p>Get a list of available models.</p> <p>Endpoint: <code>GET /models</code></p> <p>Response: <pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"meta-llama/Llama-2-7b-chat-hf\",\n      \"object\": \"model\",\n      \"created\": 1640995200,\n      \"owned_by\": \"inferneo\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#get-model","title":"Get Model","text":"<p>Get information about a specific model.</p> <p>Endpoint: <code>GET /models/{model_id}</code></p> <p>Response: <pre><code>{\n  \"id\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"object\": \"model\",\n  \"created\": 1640995200,\n  \"owned_by\": \"inferneo\",\n  \"permission\": [],\n  \"root\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"parent\": null\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#completions","title":"Completions","text":""},{"location":"api-reference/rest-api/#create-completion","title":"Create Completion","text":"<p>Generate text completions.</p> <p>Endpoint: <code>POST /completions</code></p> <p>Request Body: <pre><code>{\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"prompt\": \"Explain quantum computing\",\n  \"max_tokens\": 100,\n  \"temperature\": 0.7,\n  \"top_p\": 0.9,\n  \"frequency_penalty\": 0.0,\n  \"presence_penalty\": 0.0,\n  \"stop\": [\"\\n\", \"END\"],\n  \"stream\": false\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"id\": \"cmpl-1234567890\",\n  \"object\": \"text_completion\",\n  \"created\": 1640995200,\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"choices\": [\n    {\n      \"text\": \"Quantum computing is a revolutionary technology...\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 4,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 104\n  }\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#streaming-completions","title":"Streaming Completions","text":"<p>Get streaming completions for real-time text generation.</p> <p>Request Body: <pre><code>{\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"prompt\": \"Write a story about AI\",\n  \"max_tokens\": 200,\n  \"temperature\": 0.8,\n  \"stream\": true\n}\n</code></pre></p> <p>Response (Server-Sent Events): <pre><code>data: {\"id\":\"cmpl-123\",\"object\":\"text_completion\",\"created\":1640995200,\"choices\":[{\"text\":\"Once\",\"index\":0,\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"cmpl-123\",\"object\":\"text_completion\",\"created\":1640995200,\"choices\":[{\"text\":\" upon\",\"index\":0,\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"cmpl-123\",\"object\":\"text_completion\",\"created\":1640995200,\"choices\":[{\"text\":\" a\",\"index\":0,\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: {\"id\":\"cmpl-123\",\"object\":\"text_completion\",\"created\":1640995200,\"choices\":[{\"text\":\" time\",\"index\":0,\"logprobs\":null,\"finish_reason\":null}]}\n\ndata: [DONE]\n</code></pre></p>"},{"location":"api-reference/rest-api/#chat-completions","title":"Chat Completions","text":""},{"location":"api-reference/rest-api/#create-chat-completion","title":"Create Chat Completion","text":"<p>Generate chat-style completions with conversation history.</p> <p>Endpoint: <code>POST /chat/completions</code></p> <p>Request Body: <pre><code>{\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello, how are you?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"I'm doing well, thank you for asking! How can I help you today?\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Can you explain machine learning?\"\n    }\n  ],\n  \"max_tokens\": 150,\n  \"temperature\": 0.7,\n  \"top_p\": 0.9,\n  \"frequency_penalty\": 0.0,\n  \"presence_penalty\": 0.0,\n  \"stream\": false\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"id\": \"chatcmpl-1234567890\",\n  \"object\": \"chat.completion\",\n  \"created\": 1640995200,\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Machine learning is a subset of artificial intelligence...\"\n      },\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"completion_tokens\": 150,\n    \"total_tokens\": 195\n  }\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#streaming-chat-completions","title":"Streaming Chat Completions","text":"<p>Get streaming chat completions.</p> <p>Request Body: <pre><code>{\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Tell me a story\"\n    }\n  ],\n  \"max_tokens\": 200,\n  \"stream\": true\n}\n</code></pre></p> <p>Response (Server-Sent Events): <pre><code>data: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion\",\"created\":1640995200,\"choices\":[{\"delta\":{\"role\":\"assistant\"},\"index\":0,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion\",\"created\":1640995200,\"choices\":[{\"delta\":{\"content\":\"Once\"},\"index\":0,\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-123\",\"object\":\"chat.completion\",\"created\":1640995200,\"choices\":[{\"delta\":{\"content\":\" upon\"},\"index\":0,\"finish_reason\":null}]}\n\ndata: [DONE]\n</code></pre></p>"},{"location":"api-reference/rest-api/#embeddings","title":"Embeddings","text":""},{"location":"api-reference/rest-api/#create-embeddings","title":"Create Embeddings","text":"<p>Generate embeddings for text.</p> <p>Endpoint: <code>POST /embeddings</code></p> <p>Request Body: <pre><code>{\n  \"model\": \"text-embedding-ada-002\",\n  \"input\": \"This is a sample text for embedding\",\n  \"encoding_format\": \"float\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"embedding\": [0.1, 0.2, 0.3, ...],\n      \"index\": 0\n    }\n  ],\n  \"model\": \"text-embedding-ada-002\",\n  \"usage\": {\n    \"prompt_tokens\": 8,\n    \"total_tokens\": 8\n  }\n}\n</code></pre></p>"},{"location":"api-reference/rest-api/#parameters","title":"Parameters","text":""},{"location":"api-reference/rest-api/#common-parameters","title":"Common Parameters","text":"Parameter Type Default Description <code>model</code> string required The model to use for generation <code>max_tokens</code> integer 16 Maximum number of tokens to generate <code>temperature</code> number 1.0 Controls randomness (0.0 = deterministic, 2.0 = very random) <code>top_p</code> number 1.0 Nucleus sampling parameter <code>top_k</code> integer -1 Top-k sampling parameter <code>frequency_penalty</code> number 0.0 Penalty for frequent tokens <code>presence_penalty</code> number 0.0 Penalty for new tokens <code>stop</code> array null Stop sequences <code>stream</code> boolean false Whether to stream the response <code>logprobs</code> integer null Number of log probabilities to return <code>echo</code> boolean false Echo the prompt in the response"},{"location":"api-reference/rest-api/#advanced-parameters","title":"Advanced Parameters","text":"Parameter Type Default Description <code>best_of</code> integer 1 Number of best completions to return <code>logit_bias</code> object {} Bias for specific tokens <code>suffix</code> string null Suffix to append to completion <code>user</code> string null User identifier for abuse monitoring"},{"location":"api-reference/rest-api/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/rest-api/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"error\": {\n    \"message\": \"Error description\",\n    \"type\": \"invalid_request_error\",\n    \"param\": \"model\",\n    \"code\": \"model_not_found\"\n  }\n}\n</code></pre>"},{"location":"api-reference/rest-api/#common-error-codes","title":"Common Error Codes","text":"Code Description <code>invalid_request_error</code> Invalid request parameters <code>model_not_found</code> Model not found <code>rate_limit_exceeded</code> Rate limit exceeded <code>insufficient_quota</code> Insufficient quota <code>server_error</code> Internal server error"},{"location":"api-reference/rest-api/#rate-limiting","title":"Rate Limiting","text":"<p>Rate limits are applied per API key:</p> <ul> <li>Requests per minute: 60</li> <li>Tokens per minute: 150,000</li> <li>Requests per day: 3,500</li> </ul> <p>Rate limit headers are included in responses:</p> <pre><code>X-RateLimit-Limit: 60\nX-RateLimit-Remaining: 59\nX-RateLimit-Reset: 1640995260\n</code></pre>"},{"location":"api-reference/rest-api/#examples","title":"Examples","text":""},{"location":"api-reference/rest-api/#python-client","title":"Python Client","text":"<pre><code>import requests\nimport json\n\n# Base configuration\nbase_url = \"http://localhost:8000/v1\"\nheaders = {\n    \"Authorization\": \"Bearer your-api-key\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Text completion\ndef create_completion(prompt, model=\"meta-llama/Llama-2-7b-chat-hf\"):\n    url = f\"{base_url}/completions\"\n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"max_tokens\": 100,\n        \"temperature\": 0.7\n    }\n\n    response = requests.post(url, headers=headers, json=data)\n    return response.json()\n\n# Chat completion\ndef create_chat_completion(messages, model=\"meta-llama/Llama-2-7b-chat-hf\"):\n    url = f\"{base_url}/chat/completions\"\n    data = {\n        \"model\": model,\n        \"messages\": messages,\n        \"max_tokens\": 150,\n        \"temperature\": 0.7\n    }\n\n    response = requests.post(url, headers=headers, json=data)\n    return response.json()\n\n# Usage\ncompletion = create_completion(\"Explain quantum computing\")\nprint(completion[\"choices\"][0][\"text\"])\n\nchat_messages = [\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n]\nchat_response = create_chat_completion(chat_messages)\nprint(chat_response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"api-reference/rest-api/#javascript-client","title":"JavaScript Client","text":"<pre><code>// Base configuration\nconst baseUrl = 'http://localhost:8000/v1';\nconst headers = {\n    'Authorization': 'Bearer your-api-key',\n    'Content-Type': 'application/json'\n};\n\n// Text completion\nasync function createCompletion(prompt, model = 'meta-llama/Llama-2-7b-chat-hf') {\n    const response = await fetch(`${baseUrl}/completions`, {\n        method: 'POST',\n        headers: headers,\n        body: JSON.stringify({\n            model: model,\n            prompt: prompt,\n            max_tokens: 100,\n            temperature: 0.7\n        })\n    });\n\n    return await response.json();\n}\n\n// Chat completion\nasync function createChatCompletion(messages, model = 'meta-llama/Llama-2-7b-chat-hf') {\n    const response = await fetch(`${baseUrl}/chat/completions`, {\n        method: 'POST',\n        headers: headers,\n        body: JSON.stringify({\n            model: model,\n            messages: messages,\n            max_tokens: 150,\n            temperature: 0.7\n        })\n    });\n\n    return await response.json();\n}\n\n// Usage\ncreateCompletion(\"Explain quantum computing\")\n    .then(response =&gt; console.log(response.choices[0].text));\n\nconst messages = [\n    {role: \"user\", content: \"Hello, how are you?\"}\n];\ncreateChatCompletion(messages)\n    .then(response =&gt; console.log(response.choices[0].message.content));\n</code></pre>"},{"location":"api-reference/rest-api/#curl-examples","title":"cURL Examples","text":"<pre><code># Text completion\ncurl -X POST http://localhost:8000/v1/completions \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"prompt\": \"Explain quantum computing\",\n    \"max_tokens\": 100,\n    \"temperature\": 0.7\n  }'\n\n# Chat completion\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ],\n    \"max_tokens\": 150,\n    \"temperature\": 0.7\n  }'\n\n# Streaming completion\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Tell me a story\"}\n    ],\n    \"max_tokens\": 200,\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"api-reference/rest-api/#websocket-api","title":"WebSocket API","text":"<p>For real-time streaming, you can also use the WebSocket API:</p> <pre><code>const ws = new WebSocket('ws://localhost:8000/v1/chat/completions');\n\nws.onopen = function() {\n    ws.send(JSON.stringify({\n        model: 'meta-llama/Llama-2-7b-chat-hf',\n        messages: [\n            {role: 'user', content: 'Tell me a story'}\n        ],\n        stream: true\n    }));\n};\n\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n    if (data.choices &amp;&amp; data.choices[0].delta.content) {\n        console.log(data.choices[0].delta.content);\n    }\n};\n</code></pre> <p>For more information about the Python client, see the Python Client documentation. </p>"},{"location":"api-reference/websocket-api/","title":"WebSocket API","text":"<p>Inferneo provides a WebSocket API for real-time streaming and bidirectional communication.</p>"},{"location":"api-reference/websocket-api/#connection","title":"Connection","text":""},{"location":"api-reference/websocket-api/#websocket-url","title":"WebSocket URL","text":"<pre><code>ws://localhost:8000/v1/chat/completions\n</code></pre>"},{"location":"api-reference/websocket-api/#connection-setup","title":"Connection Setup","text":"<pre><code>const ws = new WebSocket('ws://localhost:8000/v1/chat/completions');\n\nws.onopen = function() {\n    console.log('Connected to Inferneo WebSocket API');\n};\n\nws.onerror = function(error) {\n    console.error('WebSocket error:', error);\n};\n\nws.onclose = function(event) {\n    console.log('WebSocket connection closed:', event.code, event.reason);\n};\n</code></pre>"},{"location":"api-reference/websocket-api/#authentication","title":"Authentication","text":""},{"location":"api-reference/websocket-api/#api-key-authentication","title":"API Key Authentication","text":"<pre><code>const ws = new WebSocket('ws://localhost:8000/v1/chat/completions');\n\nws.onopen = function() {\n    // Send authentication message\n    ws.send(JSON.stringify({\n        type: 'auth',\n        api_key: 'your-api-key'\n    }));\n};\n</code></pre>"},{"location":"api-reference/websocket-api/#token-based-authentication","title":"Token-based Authentication","text":"<pre><code>const ws = new WebSocket('ws://localhost:8000/v1/chat/completions?token=your-token');\n</code></pre>"},{"location":"api-reference/websocket-api/#chat-completions","title":"Chat Completions","text":""},{"location":"api-reference/websocket-api/#basic-chat-completion","title":"Basic Chat Completion","text":"<pre><code>const ws = new WebSocket('ws://localhost:8000/v1/chat/completions');\n\nws.onopen = function() {\n    // Send chat completion request\n    ws.send(JSON.stringify({\n        model: 'meta-llama/Llama-2-7b-chat-hf',\n        messages: [\n            {role: 'user', content: 'Hello, how are you?'}\n        ],\n        max_tokens: 100,\n        temperature: 0.7,\n        stream: true\n    }));\n};\n\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n\n    if (data.choices &amp;&amp; data.choices[0].delta.content) {\n        console.log(data.choices[0].delta.content);\n    }\n\n    if (data.choices &amp;&amp; data.choices[0].finish_reason) {\n        console.log('Generation completed');\n        ws.close();\n    }\n};\n</code></pre>"},{"location":"api-reference/websocket-api/#streaming-chat-with-history","title":"Streaming Chat with History","text":"<pre><code>const ws = new WebSocket('ws://localhost:8000/v1/chat/completions');\n\nconst conversation = [\n    {role: 'system', content: 'You are a helpful assistant.'},\n    {role: 'user', content: 'What is machine learning?'},\n    {role: 'assistant', content: 'Machine learning is a subset of artificial intelligence...'},\n    {role: 'user', content: 'Can you give me an example?'}\n];\n\nws.onopen = function() {\n    ws.send(JSON.stringify({\n        model: 'meta-llama/Llama-2-7b-chat-hf',\n        messages: conversation,\n        max_tokens: 150,\n        temperature: 0.7,\n        stream: true\n    }));\n};\n\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n\n    if (data.choices &amp;&amp; data.choices[0].delta.content) {\n        process.stdout.write(data.choices[0].delta.content);\n    }\n\n    if (data.choices &amp;&amp; data.choices[0].finish_reason) {\n        console.log('\\nGeneration completed');\n        ws.close();\n    }\n};\n</code></pre>"},{"location":"api-reference/websocket-api/#text-completions","title":"Text Completions","text":""},{"location":"api-reference/websocket-api/#streaming-text-completion","title":"Streaming Text Completion","text":"<pre><code>const ws = new WebSocket('ws://localhost:8000/v1/completions');\n\nws.onopen = function() {\n    ws.send(JSON.stringify({\n        model: 'meta-llama/Llama-2-7b-chat-hf',\n        prompt: 'Explain quantum computing',\n        max_tokens: 200,\n        temperature: 0.8,\n        stream: true\n    }));\n};\n\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n\n    if (data.choices &amp;&amp; data.choices[0].text) {\n        process.stdout.write(data.choices[0].text);\n    }\n\n    if (data.choices &amp;&amp; data.choices[0].finish_reason) {\n        console.log('\\nCompletion finished');\n        ws.close();\n    }\n};\n</code></pre>"},{"location":"api-reference/websocket-api/#advanced-features","title":"Advanced Features","text":""},{"location":"api-reference/websocket-api/#multiple-concurrent-requests","title":"Multiple Concurrent Requests","text":"<pre><code>class InferneoWebSocketClient {\n    constructor(url) {\n        this.url = url;\n        this.connections = new Map();\n        this.requestId = 0;\n    }\n\n    async sendRequest(request) {\n        const id = ++this.requestId;\n\n        return new Promise((resolve, reject) =&gt; {\n            const ws = new WebSocket(this.url);\n            const response = {text: '', id: id};\n\n            ws.onopen = function() {\n                ws.send(JSON.stringify({\n                    ...request,\n                    request_id: id\n                }));\n            };\n\n            ws.onmessage = function(event) {\n                const data = JSON.parse(event.data);\n\n                if (data.choices &amp;&amp; data.choices[0].delta.content) {\n                    response.text += data.choices[0].delta.content;\n                }\n\n                if (data.choices &amp;&amp; data.choices[0].finish_reason) {\n                    resolve(response);\n                    ws.close();\n                }\n            };\n\n            ws.onerror = function(error) {\n                reject(error);\n            };\n\n            this.connections.set(id, ws);\n        });\n    }\n\n    closeAll() {\n        this.connections.forEach(ws =&gt; ws.close());\n        this.connections.clear();\n    }\n}\n\n// Usage\nconst client = new InferneoWebSocketClient('ws://localhost:8000/v1/chat/completions');\n\n// Send multiple requests\nconst requests = [\n    {\n        model: 'meta-llama/Llama-2-7b-chat-hf',\n        messages: [{role: 'user', content: 'Explain AI'}],\n        max_tokens: 100,\n        stream: true\n    },\n    {\n        model: 'meta-llama/Llama-2-7b-chat-hf',\n        messages: [{role: 'user', content: 'What is ML?'}],\n        max_tokens: 100,\n        stream: true\n    }\n];\n\nPromise.all(requests.map(req =&gt; client.sendRequest(req)))\n    .then(responses =&gt; {\n        responses.forEach(res =&gt; console.log(`Response ${res.id}:`, res.text));\n    })\n    .finally(() =&gt; client.closeAll());\n</code></pre>"},{"location":"api-reference/websocket-api/#error-handling","title":"Error Handling","text":"<pre><code>const ws = new WebSocket('ws://localhost:8000/v1/chat/completions');\n\nws.onopen = function() {\n    ws.send(JSON.stringify({\n        model: 'meta-llama/Llama-2-7b-chat-hf',\n        messages: [{role: 'user', content: 'Hello'}],\n        max_tokens: 100,\n        stream: true\n    }));\n};\n\nws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n\n    // Check for errors\n    if (data.error) {\n        console.error('API Error:', data.error);\n        ws.close();\n        return;\n    }\n\n    if (data.choices &amp;&amp; data.choices[0].delta.content) {\n        console.log(data.choices[0].delta.content);\n    }\n\n    if (data.choices &amp;&amp; data.choices[0].finish_reason) {\n        console.log('Completed');\n        ws.close();\n    }\n};\n\nws.onerror = function(error) {\n    console.error('WebSocket error:', error);\n};\n\nws.onclose = function(event) {\n    if (event.code !== 1000) {\n        console.error('Connection closed unexpectedly:', event.code, event.reason);\n    }\n};\n</code></pre>"},{"location":"api-reference/websocket-api/#reconnection-logic","title":"Reconnection Logic","text":"<pre><code>class ReconnectingWebSocket {\n    constructor(url, options = {}) {\n        this.url = url;\n        this.options = {\n            maxReconnectAttempts: 5,\n            reconnectInterval: 1000,\n            ...options\n        };\n        this.reconnectAttempts = 0;\n        this.isConnecting = false;\n        this.messageQueue = [];\n        this.connect();\n    }\n\n    connect() {\n        if (this.isConnecting) return;\n\n        this.isConnecting = true;\n        this.ws = new WebSocket(this.url);\n\n        this.ws.onopen = () =&gt; {\n            this.isConnecting = false;\n            this.reconnectAttempts = 0;\n            console.log('Connected to WebSocket');\n\n            // Send queued messages\n            while (this.messageQueue.length &gt; 0) {\n                this.ws.send(this.messageQueue.shift());\n            }\n        };\n\n        this.ws.onclose = (event) =&gt; {\n            this.isConnecting = false;\n\n            if (event.code !== 1000 &amp;&amp; this.reconnectAttempts &lt; this.options.maxReconnectAttempts) {\n                console.log(`Connection closed, attempting to reconnect (${this.reconnectAttempts + 1}/${this.options.maxReconnectAttempts})`);\n                this.reconnectAttempts++;\n\n                setTimeout(() =&gt; {\n                    this.connect();\n                }, this.options.reconnectInterval * this.reconnectAttempts);\n            }\n        };\n\n        this.ws.onerror = (error) =&gt; {\n            console.error('WebSocket error:', error);\n        };\n    }\n\n    send(data) {\n        if (this.ws &amp;&amp; this.ws.readyState === WebSocket.OPEN) {\n            this.ws.send(typeof data === 'string' ? data : JSON.stringify(data));\n        } else {\n            this.messageQueue.push(typeof data === 'string' ? data : JSON.stringify(data));\n        }\n    }\n\n    close() {\n        if (this.ws) {\n            this.ws.close(1000);\n        }\n    }\n}\n\n// Usage\nconst ws = new ReconnectingWebSocket('ws://localhost:8000/v1/chat/completions');\n\nws.ws.onmessage = function(event) {\n    const data = JSON.parse(event.data);\n    if (data.choices &amp;&amp; data.choices[0].delta.content) {\n        console.log(data.choices[0].delta.content);\n    }\n};\n\n// Send message (will be queued if not connected)\nws.send({\n    model: 'meta-llama/Llama-2-7b-chat-hf',\n    messages: [{role: 'user', content: 'Hello'}],\n    max_tokens: 100,\n    stream: true\n});\n</code></pre>"},{"location":"api-reference/websocket-api/#python-websocket-client","title":"Python WebSocket Client","text":"<pre><code>import asyncio\nimport websockets\nimport json\n\nclass InferneoWebSocketClient:\n    def __init__(self, url):\n        self.url = url\n        self.websocket = None\n\n    async def connect(self):\n        self.websocket = await websockets.connect(self.url)\n\n    async def send_request(self, request):\n        if not self.websocket:\n            await self.connect()\n\n        await self.websocket.send(json.dumps(request))\n\n        full_response = \"\"\n        async for message in self.websocket:\n            data = json.loads(message)\n\n            if \"error\" in data:\n                raise Exception(f\"API Error: {data['error']}\")\n\n            if \"choices\" in data and data[\"choices\"][0].get(\"delta\", {}).get(\"content\"):\n                content = data[\"choices\"][0][\"delta\"][\"content\"]\n                full_response += content\n                print(content, end=\"\", flush=True)\n\n            if \"choices\" in data and data[\"choices\"][0].get(\"finish_reason\"):\n                print()  # New line\n                break\n\n        return full_response\n\n    async def close(self):\n        if self.websocket:\n            await self.websocket.close()\n\n# Usage\nasync def main():\n    client = InferneoWebSocketClient(\"ws://localhost:8000/v1/chat/completions\")\n\n    try:\n        request = {\n            \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Tell me a story\"}\n            ],\n            \"max_tokens\": 200,\n            \"temperature\": 0.8,\n            \"stream\": True\n        }\n\n        response = await client.send_request(request)\n        print(f\"\\nFull response: {response}\")\n\n    finally:\n        await client.close()\n\n# Run the async function\nasyncio.run(main())\n</code></pre>"},{"location":"api-reference/websocket-api/#message-format","title":"Message Format","text":""},{"location":"api-reference/websocket-api/#request-format","title":"Request Format","text":"<pre><code>{\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Hello\"}\n  ],\n  \"max_tokens\": 100,\n  \"temperature\": 0.7,\n  \"top_p\": 0.9,\n  \"frequency_penalty\": 0.0,\n  \"presence_penalty\": 0.0,\n  \"stop\": [\"\\n\", \"END\"],\n  \"stream\": true,\n  \"request_id\": \"unique-request-id\"\n}\n</code></pre>"},{"location":"api-reference/websocket-api/#response-format","title":"Response Format","text":"<pre><code>{\n  \"id\": \"chatcmpl-1234567890\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1640995200,\n  \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"delta\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello\"\n      },\n      \"finish_reason\": null\n    }\n  ]\n}\n</code></pre>"},{"location":"api-reference/websocket-api/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"error\": {\n    \"message\": \"Error description\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n</code></pre>"},{"location":"api-reference/websocket-api/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/websocket-api/#connection-management","title":"Connection Management","text":"<pre><code>// Keep connections alive with ping/pong\nconst ws = new WebSocket('ws://localhost:8000/v1/chat/completions');\n\n// Send ping every 30 seconds\nconst pingInterval = setInterval(() =&gt; {\n    if (ws.readyState === WebSocket.OPEN) {\n        ws.send(JSON.stringify({type: 'ping'}));\n    }\n}, 30000);\n\nws.onclose = function() {\n    clearInterval(pingInterval);\n};\n</code></pre>"},{"location":"api-reference/websocket-api/#rate-limiting","title":"Rate Limiting","text":"<pre><code>class RateLimitedWebSocket {\n    constructor(url, rateLimit = 10) { // 10 requests per second\n        this.url = url;\n        this.rateLimit = rateLimit;\n        this.requestQueue = [];\n        this.lastRequestTime = 0;\n        this.connect();\n    }\n\n    async sendRequest(request) {\n        return new Promise((resolve) =&gt; {\n            this.requestQueue.push({request, resolve});\n            this.processQueue();\n        });\n    }\n\n    async processQueue() {\n        if (this.requestQueue.length === 0) return;\n\n        const now = Date.now();\n        const timeSinceLastRequest = now - this.lastRequestTime;\n        const minInterval = 1000 / this.rateLimit;\n\n        if (timeSinceLastRequest &lt; minInterval) {\n            setTimeout(() =&gt; this.processQueue(), minInterval - timeSinceLastRequest);\n            return;\n        }\n\n        const {request, resolve} = this.requestQueue.shift();\n        this.lastRequestTime = now;\n\n        // Send the request\n        this.ws.send(JSON.stringify(request));\n        resolve();\n    }\n}\n</code></pre>"},{"location":"api-reference/websocket-api/#error-recovery","title":"Error Recovery","text":"<pre><code>class ResilientWebSocket {\n    constructor(url) {\n        this.url = url;\n        this.maxRetries = 3;\n        this.retryDelay = 1000;\n        this.connect();\n    }\n\n    async sendWithRetry(request, retries = 0) {\n        try {\n            return await this.sendRequest(request);\n        } catch (error) {\n            if (retries &lt; this.maxRetries) {\n                console.log(`Retry ${retries + 1}/${this.maxRetries}`);\n                await new Promise(resolve =&gt; setTimeout(resolve, this.retryDelay * (retries + 1)));\n                return this.sendWithRetry(request, retries + 1);\n            } else {\n                throw error;\n            }\n        }\n    }\n}\n</code></pre> <p>For more information about the REST API, see the REST API documentation. </p>"},{"location":"cli-reference/commands/","title":"CLI Reference","text":""},{"location":"cli-reference/commands/#overview","title":"Overview","text":"<p>Command-line interface reference for Inferneo.</p>"},{"location":"cli-reference/commands/#basic-commands","title":"Basic Commands","text":""},{"location":"cli-reference/commands/#start-server","title":"Start Server","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"cli-reference/commands/#show-help","title":"Show Help","text":"<pre><code>inferneo --help\n</code></pre>"},{"location":"cli-reference/commands/#next-steps","title":"Next Steps","text":"<ul> <li>Environment Variables </li> </ul>"},{"location":"cli-reference/environment-variables/","title":"Environment Variables","text":""},{"location":"cli-reference/environment-variables/#overview","title":"Overview","text":"<p>Environment variables for configuring Inferneo CLI.</p>"},{"location":"cli-reference/environment-variables/#common-variables","title":"Common Variables","text":"<ul> <li><code>INFERNEO_MODEL_PATH</code>: Path to model</li> <li><code>INFERNEO_PORT</code>: Server port</li> <li><code>INFERNEO_LOG_LEVEL</code>: Logging level</li> </ul>"},{"location":"cli-reference/environment-variables/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Commands </li> </ul>"},{"location":"community/faq/","title":"FAQ","text":"<p>Frequently Asked Questions about Inferneo.</p>"},{"location":"community/faq/#general","title":"General","text":"<p>Q: What is Inferneo? A: Inferneo is a high-performance inference server for LLMs and multimodal models.</p> <p>Q: What models are supported? A: Most HuggingFace Transformers, ONNX, and select vision/multimodal models.</p> <p>Q: Is GPU required? A: GPU is recommended for best performance, but CPU is supported for smaller models.</p>"},{"location":"community/faq/#usage","title":"Usage","text":"<p>Q: How do I install Inferneo? A: See the Installation Guide.</p> <p>Q: How do I run the server? A: Use <code>inferneo serve --model &lt;model_id&gt;</code>.</p> <p>Q: How do I use quantization? A: Pass <code>--quantization awq</code> or <code>gptq</code> or <code>squeezellm</code> to the server.</p>"},{"location":"community/faq/#troubleshooting","title":"Troubleshooting","text":"<p>Q: The server won't start. What should I check? A: Ensure your model path is correct and you have enough GPU memory.</p> <p>Q: How do I report a bug? A: Open a GitHub Issue with details and logs.</p>"},{"location":"community/faq/#next-steps","title":"Next Steps","text":"<ul> <li>Roadmap</li> <li>Releases </li> </ul>"},{"location":"community/releases/","title":"Releases","text":"<p>This page lists the major releases and changelog for Inferneo.</p>"},{"location":"community/releases/#v020-2024-06-10","title":"v0.2.0 (2024-06-10)","text":"<ul> <li>Initial public release</li> <li>Python client</li> <li>REST and WebSocket APIs</li> <li>Quantization (AWQ, GPTQ, SqueezeLLM)</li> <li>Batch scheduling</li> <li>Vision and multimodal support</li> </ul>"},{"location":"community/releases/#v010-2024-05-01","title":"v0.1.0 (2024-05-01)","text":"<ul> <li>Internal alpha</li> <li>Basic text generation</li> <li>Model loading and management</li> </ul>"},{"location":"community/releases/#release-process","title":"Release Process","text":"<ul> <li>Releases are tagged on GitHub</li> <li>See GitHub Releases for details</li> </ul>"},{"location":"community/releases/#next-steps","title":"Next Steps","text":"<ul> <li>Roadmap</li> <li>FAQ </li> </ul>"},{"location":"community/roadmap/","title":"Roadmap","text":"<p>This page outlines the planned features and improvements for Inferneo.</p>"},{"location":"community/roadmap/#upcoming-features","title":"Upcoming Features","text":"<ul> <li>Model hot-swapping</li> <li>Advanced quantization support</li> <li>Improved distributed inference</li> <li>Enhanced monitoring and metrics</li> <li>Plugin marketplace</li> <li>More vision/multimodal models</li> </ul>"},{"location":"community/roadmap/#in-progress","title":"In Progress","text":"<ul> <li>Web UI for management</li> <li>Kubernetes Helm charts</li> <li>Performance benchmarking tools</li> </ul>"},{"location":"community/roadmap/#recently-released","title":"Recently Released","text":"<ul> <li>Python client</li> <li>REST and WebSocket APIs</li> <li>Quantization (AWQ, GPTQ, SqueezeLLM)</li> <li>Batch scheduling</li> </ul>"},{"location":"community/roadmap/#how-to-contribute","title":"How to Contribute","text":"<ul> <li>Suggest features via GitHub Issues</li> <li>Upvote features you want prioritized</li> </ul>"},{"location":"community/roadmap/#next-steps","title":"Next Steps","text":"<ul> <li>Releases</li> <li>FAQ </li> </ul>"},{"location":"developer-guide/architecture/","title":"Architecture","text":""},{"location":"developer-guide/architecture/#overview","title":"Overview","text":""},{"location":"developer-guide/architecture/#core-components","title":"Core Components","text":""},{"location":"developer-guide/architecture/#model-engine","title":"Model Engine","text":""},{"location":"developer-guide/architecture/#api-layer","title":"API Layer","text":""},{"location":"developer-guide/architecture/#scheduler","title":"Scheduler","text":""},{"location":"developer-guide/architecture/#design-principles","title":"Design Principles","text":""},{"location":"developer-guide/architecture/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"developer-guide/architecture/#single-node","title":"Single Node","text":""},{"location":"developer-guide/architecture/#distributed","title":"Distributed","text":""},{"location":"developer-guide/architecture/#configuration","title":"Configuration","text":""},{"location":"developer-guide/architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"developer-guide/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Inferneo!</p>"},{"location":"developer-guide/contributing/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the repository</li> <li>Create a new branch for your feature or bugfix</li> <li>Make your changes</li> <li>Write tests if applicable</li> <li>Update documentation if needed</li> <li>Submit a pull request</li> </ol>"},{"location":"developer-guide/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP8 for Python code</li> <li>Use clear, descriptive commit messages</li> </ul>"},{"location":"developer-guide/contributing/#reporting-issues","title":"Reporting Issues","text":"<ul> <li>Use GitHub Issues to report bugs or request features</li> <li>Provide as much detail as possible</li> </ul>"},{"location":"developer-guide/contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/deepdik/inferneo.git\ncd inferneo\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"developer-guide/contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"developer-guide/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Follow the Contributor Covenant</li> </ul>"},{"location":"developer-guide/contributing/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture</li> <li>Custom Models </li> </ul>"},{"location":"developer-guide/custom-models/","title":"Custom Models","text":"<p>This page explains how to add and serve custom models with Inferneo.</p>"},{"location":"developer-guide/custom-models/#adding-a-custom-model","title":"Adding a Custom Model","text":"<ol> <li>Prepare your model in a supported format (e.g., HuggingFace Transformers, ONNX)</li> <li>Place the model files in a directory accessible to the server</li> <li>Update the configuration to include your model</li> </ol>"},{"location":"developer-guide/custom-models/#example-huggingface-model","title":"Example: HuggingFace Model","text":"<pre><code>inferneo serve --model my-org/my-custom-model\n</code></pre>"},{"location":"developer-guide/custom-models/#example-onnx-model","title":"Example: ONNX Model","text":"<pre><code>inferneo serve --model /path/to/model.onnx\n</code></pre>"},{"location":"developer-guide/custom-models/#model-configuration","title":"Model Configuration","text":"<p>You can specify model parameters in <code>config.yaml</code>:</p> <pre><code>model: my-org/my-custom-model\nmax_model_len: 4096\ngpu_memory_utilization: 0.9\n</code></pre>"},{"location":"developer-guide/custom-models/#prepost-processing","title":"Pre/Post-Processing","text":"<ul> <li>Implement custom pre-processing or post-processing using the plugin system</li> </ul>"},{"location":"developer-guide/custom-models/#testing-your-model","title":"Testing Your Model","text":"<ul> <li>Use the Python client or REST API to send test requests</li> </ul>"},{"location":"developer-guide/custom-models/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Tuning - Advanced optimization techniques</li> <li>Contributing </li> </ul>"},{"location":"examples/chat-completion/","title":"Chat Completion Example","text":"<p>This example demonstrates how to use Inferneo for chat-based completions.</p>"},{"location":"examples/chat-completion/#python-client-example","title":"Python Client Example","text":"<pre><code>from inferneo import InferneoClient\n\nclient = InferneoClient(\"http://localhost:8000\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n    {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of AI...\"},\n    {\"role\": \"user\", \"content\": \"Can you give me an example?\"}\n]\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    messages=messages,\n    max_tokens=150,\n    temperature=0.7\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/chat-completion/#rest-api-example","title":"REST API Example","text":"<pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n    ],\n    \"max_tokens\": 150\n  }'\n</code></pre>"},{"location":"examples/chat-completion/#streaming-example","title":"Streaming Example","text":"<pre><code>stream = client.chat.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    messages=messages,\n    max_tokens=150,\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"examples/chat-completion/#next-steps","title":"Next Steps","text":"<ul> <li>Streaming</li> <li>Batching </li> </ul>"},{"location":"examples/embeddings/","title":"Embeddings Example","text":"<p>This example demonstrates how to use Inferneo to generate embeddings for text data.</p>"},{"location":"examples/embeddings/#python-client-example","title":"Python Client Example","text":"<pre><code>from inferneo import InferneoClient\n\nclient = InferneoClient(\"http://localhost:8000\")\n\ntexts = [\n    \"What is machine learning?\",\n    \"Explain deep learning.\",\n    \"Describe neural networks.\"\n]\n\n# Generate embeddings\nresponse = client.embeddings.create(\n    model=\"meta-llama/Llama-2-7b-embeddings\",\n    input=texts\n)\n\n# Access embeddings\nfor i, embedding in enumerate(response.data):\n    print(f\"Text {i+1} embedding: {embedding['embedding'][:5]} ...\")\n</code></pre>"},{"location":"examples/embeddings/#rest-api-example","title":"REST API Example","text":"<pre><code>curl -X POST http://localhost:8000/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-embeddings\",\n    \"input\": [\n      \"What is machine learning?\",\n      \"Explain deep learning.\"\n    ]\n  }'\n</code></pre>"},{"location":"examples/embeddings/#use-cases","title":"Use Cases","text":"<ul> <li>Semantic search</li> <li>Clustering</li> <li>Similarity comparison</li> </ul>"},{"location":"examples/embeddings/#next-steps","title":"Next Steps","text":"<ul> <li>Text Generation</li> <li>Vision Models </li> </ul>"},{"location":"examples/multimodal/","title":"Multimodal Example","text":"<p>This example demonstrates how to use Inferneo for multimodal inference (text + image).</p>"},{"location":"examples/multimodal/#python-client-example","title":"Python Client Example","text":"<pre><code>from inferneo import InferneoClient\n\nclient = InferneoClient(\"http://localhost:8000\")\n\n# Load image\nwith open(\"dog.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\n# Multimodal prompt\nprompt = \"Describe the image and its mood.\"\n\nresponse = client.multimodal.create(\n    model=\"openai/blip-2\",\n    prompt=prompt,\n    image=image_bytes\n)\n\nprint(\"Response:\", response.choices[0].text)\n</code></pre>"},{"location":"examples/multimodal/#rest-api-example","title":"REST API Example","text":"<pre><code>curl -X POST http://localhost:8000/v1/multimodal \\\n  -H \"Content-Type: application/json\" \\\n  --data-binary @dog.jpg \\\n  -H \"Content-Type: application/octet-stream\" \\\n  -H \"Prompt: Describe the image and its mood.\" \\\n  -H \"Model: openai/blip-2\"\n</code></pre>"},{"location":"examples/multimodal/#use-cases","title":"Use Cases","text":"<ul> <li>Visual question answering</li> <li>Image-text retrieval</li> <li>Multimodal chatbots</li> </ul>"},{"location":"examples/multimodal/#next-steps","title":"Next Steps","text":"<ul> <li>Vision Models</li> <li>Text Generation </li> </ul>"},{"location":"examples/text-generation/","title":"Text Generation Example","text":""},{"location":"examples/text-generation/#overview","title":"Overview","text":"<p>Example for text generation with Inferneo.</p>"},{"location":"examples/text-generation/#basic-usage","title":"Basic Usage","text":"<pre><code>import requests\n\nresponse = requests.post(\"http://localhost:8000/v1/chat/completions\", \n    json={\n        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }\n)\nprint(response.json())\n</code></pre>"},{"location":"examples/text-generation/#next-steps","title":"Next Steps","text":"<ul> <li>Embeddings Example</li> <li>Vision Models Example</li> <li>Multimodal Example </li> </ul>"},{"location":"examples/vision-models/","title":"Vision Models Example","text":"<p>This example demonstrates how to use Inferneo for vision model inference (e.g., image captioning, classification).</p>"},{"location":"examples/vision-models/#python-client-example","title":"Python Client Example","text":"<pre><code>from inferneo import InferneoClient\n\nclient = InferneoClient(\"http://localhost:8000\")\n\n# Load image\nwith open(\"cat.jpg\", \"rb\") as f:\n    image_bytes = f.read()\n\n# Run vision model\nresponse = client.vision.create(\n    model=\"openai/clip-vit-base-patch16\",\n    image=image_bytes\n)\n\nprint(\"Prediction:\", response.choices[0].text)\n</code></pre>"},{"location":"examples/vision-models/#rest-api-example","title":"REST API Example","text":"<pre><code>curl -X POST http://localhost:8000/v1/vision \\\n  -H \"Content-Type: application/json\" \\\n  --data-binary @cat.jpg \\\n  -H \"Content-Type: application/octet-stream\" \\\n  -H \"Model: openai/clip-vit-base-patch16\"\n</code></pre>"},{"location":"examples/vision-models/#use-cases","title":"Use Cases","text":"<ul> <li>Image captioning</li> <li>Image classification</li> <li>Multimodal search</li> </ul>"},{"location":"examples/vision-models/#next-steps","title":"Next Steps","text":"<ul> <li>Embeddings</li> <li>Multimodal </li> </ul>"},{"location":"user-guide/batching/","title":"Batching","text":""},{"location":"user-guide/batching/#overview","title":"Overview","text":"<p>Guide for optimizing performance with request batching.</p>"},{"location":"user-guide/batching/#basic-batching","title":"Basic Batching","text":""},{"location":"user-guide/batching/#enable-batching","title":"Enable Batching","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf --max-batch-size 32\n</code></pre>"},{"location":"user-guide/batching/#batch-requests","title":"Batch Requests","text":"<pre><code>import requests\n\n# Multiple requests in batch\nrequests_data = [\n    {\"model\": \"meta-llama/Llama-2-7b-chat-hf\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello 1\"}]},\n    {\"model\": \"meta-llama/Llama-2-7b-chat-hf\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello 2\"}]},\n    {\"model\": \"meta-llama/Llama-2-7b-chat-hf\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello 3\"}]}\n]\n\nresponse = requests.post(\"http://localhost:8000/v1/chat/completions\", json=requests_data)\n</code></pre>"},{"location":"user-guide/batching/#configuration","title":"Configuration","text":""},{"location":"user-guide/batching/#batch-size-tuning","title":"Batch Size Tuning","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf --max-batch-size 16\n</code></pre>"},{"location":"user-guide/batching/#concurrent-requests","title":"Concurrent Requests","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf --max-concurrent-requests 100\n</code></pre>"},{"location":"user-guide/batching/#next-steps","title":"Next Steps","text":"<ul> <li>Streaming</li> <li>Quantization</li> <li>Distributed Inference</li> <li>Performance Tuning guide </li> </ul>"},{"location":"user-guide/distributed-inference/","title":"Distributed Inference","text":"<p>Distributed inference allows you to scale Inferneo across multiple machines and GPUs to handle high-throughput workloads and large models.</p>"},{"location":"user-guide/distributed-inference/#overview","title":"Overview","text":"<p>Distributed inference enables horizontal scaling by distributing model layers, attention heads, and requests across multiple nodes, providing:</p> <ul> <li>Higher throughput for production workloads</li> <li>Larger model support beyond single GPU memory limits</li> <li>Fault tolerance with redundant nodes</li> <li>Cost optimization through efficient resource utilization</li> </ul>"},{"location":"user-guide/distributed-inference/#architecture","title":"Architecture","text":""},{"location":"user-guide/distributed-inference/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>Tensor parallelism splits model layers across multiple GPUs:</p> <pre><code>from inferneo import Inferneo\n\n# Configure tensor parallelism across 4 GPUs\nclient = Inferneo(\"http://localhost:8000\")\n\nclient.set_config(\n    tensor_parallel_size=4,  # Split across 4 GPUs\n    gpu_memory_utilization=0.8,\n    max_model_len=4096\n)\n\n# Load large model with tensor parallelism\nresponse = client.generate(\n    \"Explain distributed computing\",\n    model=\"meta-llama/Llama-2-70b-chat-hf\"\n)\n</code></pre>"},{"location":"user-guide/distributed-inference/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Pipeline parallelism distributes model layers across different machines:</p> <pre><code>from inferneo import Inferneo\n\n# Configure pipeline parallelism\nclient = Inferneo(\"http://localhost:8000\")\n\nclient.set_config(\n    pipeline_parallel_size=2,  # 2 pipeline stages\n    tensor_parallel_size=2,    # 2 GPUs per stage\n    max_model_len=4096\n)\n\n# Load model with pipeline parallelism\nresponse = client.generate(\n    \"Write a comprehensive guide to AI\",\n    model=\"meta-llama/Llama-2-70b-chat-hf\"\n)\n</code></pre>"},{"location":"user-guide/distributed-inference/#multi-node-setup","title":"Multi-Node Setup","text":""},{"location":"user-guide/distributed-inference/#coordinator-configuration","title":"Coordinator Configuration","text":"<pre><code># coordinator.yaml\ncoordinator:\n  host: \"0.0.0.0\"\n  port: 8000\n  workers:\n    - host: \"192.168.1.10\"\n      port: 8001\n      gpus: [0, 1]\n    - host: \"192.168.1.11\"\n      port: 8002\n      gpus: [0, 1]\n    - host: \"192.168.1.12\"\n      port: 8003\n      gpus: [0, 1]\n\nmodel:\n  name: \"meta-llama/Llama-2-70b-chat-hf\"\n  tensor_parallel_size: 6\n  pipeline_parallel_size: 3\n  max_model_len: 4096\n</code></pre>"},{"location":"user-guide/distributed-inference/#worker-configuration","title":"Worker Configuration","text":"<pre><code># worker.yaml\nworker:\n  host: \"0.0.0.0\"\n  port: 8001\n  coordinator_host: \"192.168.1.10\"\n  coordinator_port: 8000\n  gpu_ids: [0, 1]\n  model_cache_dir: \"/path/to/model/cache\"\n</code></pre>"},{"location":"user-guide/distributed-inference/#starting-distributed-cluster","title":"Starting Distributed Cluster","text":"<pre><code># Start coordinator\ninferneo serve --config coordinator.yaml --role coordinator\n\n# Start workers (on different machines)\ninferneo serve --config worker.yaml --role worker\n</code></pre>"},{"location":"user-guide/distributed-inference/#client-configuration","title":"Client Configuration","text":""},{"location":"user-guide/distributed-inference/#connecting-to-distributed-cluster","title":"Connecting to Distributed Cluster","text":"<pre><code>from inferneo import Inferneo\n\n# Connect to coordinator\nclient = Inferneo(\"http://192.168.1.10:8000\")\n\n# Configure for distributed inference\nclient.set_config(\n    tensor_parallel_size=6,\n    pipeline_parallel_size=3,\n    max_model_len=4096,\n    gpu_memory_utilization=0.8\n)\n\n# Generate with distributed model\nresponse = client.generate(\n    \"Explain the benefits of distributed computing\",\n    model=\"meta-llama/Llama-2-70b-chat-hf\"\n)\n</code></pre>"},{"location":"user-guide/distributed-inference/#load-balancing","title":"Load Balancing","text":"<pre><code>from inferneo import Inferneo\nimport random\n\nclass LoadBalancedClient:\n    def __init__(self, coordinator_hosts):\n        self.coordinators = coordinator_hosts\n        self.clients = [Inferneo(f\"http://{host}\") for host in coordinator_hosts]\n\n    def generate(self, prompt, **kwargs):\n        # Simple round-robin load balancing\n        client = random.choice(self.clients)\n        return client.generate(prompt, **kwargs)\n\n    def generate_batch(self, prompts, **kwargs):\n        # Distribute batch across coordinators\n        results = []\n        for i, prompt in enumerate(prompts):\n            client = self.clients[i % len(self.clients)]\n            result = client.generate(prompt, **kwargs)\n            results.append(result)\n        return results\n\n# Usage\ncoordinator_hosts = [\n    \"192.168.1.10:8000\",\n    \"192.168.1.11:8000\",\n    \"192.168.1.12:8000\"\n]\n\nlb_client = LoadBalancedClient(coordinator_hosts)\nresponse = lb_client.generate(\"Explain load balancing\")\n</code></pre>"},{"location":"user-guide/distributed-inference/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/distributed-inference/#optimal-configuration","title":"Optimal Configuration","text":"<pre><code>from inferneo import Inferneo\nimport time\nimport psutil\n\ndef benchmark_distributed_configs():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Test different configurations\n    configs = [\n        {\"tensor_parallel_size\": 1, \"pipeline_parallel_size\": 1},\n        {\"tensor_parallel_size\": 2, \"pipeline_parallel_size\": 1},\n        {\"tensor_parallel_size\": 4, \"pipeline_parallel_size\": 1},\n        {\"tensor_parallel_size\": 2, \"pipeline_parallel_size\": 2},\n        {\"tensor_parallel_size\": 4, \"pipeline_parallel_size\": 2}\n    ]\n\n    test_prompt = \"Write a detailed explanation of distributed systems\"\n    results = {}\n\n    for config in configs:\n        print(f\"Testing config: {config}\")\n\n        # Apply configuration\n        client.set_config(**config)\n\n        # Measure performance\n        start_time = time.time()\n        response = client.generate(test_prompt)\n        latency = time.time() - start_time\n\n        # Measure memory usage\n        memory_usage = psutil.virtual_memory().percent\n\n        results[str(config)] = {\n            \"latency\": latency,\n            \"memory_usage\": memory_usage,\n            \"response_length\": len(response.generated_text)\n        }\n\n    # Print results\n    print(\"\\nPerformance Comparison:\")\n    print(\"-\" * 60)\n    for config, metrics in results.items():\n        print(f\"{config}: {metrics['latency']:.3f}s, {metrics['memory_usage']:.1f}% memory\")\n\n    return results\n\n# Run benchmark\nbenchmark_distributed_configs()\n</code></pre>"},{"location":"user-guide/distributed-inference/#memory-management","title":"Memory Management","text":"<pre><code>from inferneo import Inferneo\nimport gc\n\ndef memory_efficient_distributed():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Configure for memory efficiency\n    client.set_config(\n        tensor_parallel_size=4,\n        pipeline_parallel_size=2,\n        gpu_memory_utilization=0.7,  # Conservative memory usage\n        max_model_len=2048,  # Reduce context length\n        swap_space=8  # Enable swap space\n    )\n\n    # Generate with memory monitoring\n    import psutil\n    memory_before = psutil.virtual_memory().used\n\n    response = client.generate(\"Explain memory management in distributed systems\")\n\n    memory_after = psutil.virtual_memory().used\n    memory_used = (memory_after - memory_before) / (1024 * 1024)  # MB\n\n    print(f\"Memory used: {memory_used:.1f} MB\")\n\n    # Force garbage collection\n    gc.collect()\n\n    return response\n\n# Usage\nresponse = memory_efficient_distributed()\n</code></pre>"},{"location":"user-guide/distributed-inference/#fault-tolerance","title":"Fault Tolerance","text":""},{"location":"user-guide/distributed-inference/#health-monitoring","title":"Health Monitoring","text":"<pre><code>import requests\nimport time\nfrom inferneo import Inferneo\n\nclass FaultTolerantClient:\n    def __init__(self, coordinator_hosts):\n        self.coordinators = coordinator_hosts\n        self.healthy_coordinators = coordinator_hosts.copy()\n        self.clients = {}\n        self._init_clients()\n\n    def _init_clients(self):\n        for host in self.coordinators:\n            try:\n                self.clients[host] = Inferneo(f\"http://{host}\")\n                # Test connection\n                self.clients[host].generate(\"test\", max_tokens=1)\n            except Exception as e:\n                print(f\"Failed to connect to {host}: {e}\")\n                if host in self.healthy_coordinators:\n                    self.healthy_coordinators.remove(host)\n\n    def _check_health(self, host):\n        try:\n            response = requests.get(f\"http://{host}/health\", timeout=5)\n            return response.status_code == 200\n        except:\n            return False\n\n    def generate(self, prompt, **kwargs):\n        # Try healthy coordinators\n        for host in self.healthy_coordinators:\n            try:\n                return self.clients[host].generate(prompt, **kwargs)\n            except Exception as e:\n                print(f\"Error with {host}: {e}\")\n                if not self._check_health(host):\n                    self.healthy_coordinators.remove(host)\n\n        # If all failed, try to reconnect\n        self._init_clients()\n        if self.healthy_coordinators:\n            return self.generate(prompt, **kwargs)\n        else:\n            raise Exception(\"All coordinators are unavailable\")\n\n# Usage\ncoordinator_hosts = [\n    \"192.168.1.10:8000\",\n    \"192.168.1.11:8000\",\n    \"192.168.1.12:8000\"\n]\n\nft_client = FaultTolerantClient(coordinator_hosts)\nresponse = ft_client.generate(\"Explain fault tolerance\")\n</code></pre>"},{"location":"user-guide/distributed-inference/#automatic-failover","title":"Automatic Failover","text":"<pre><code>from inferneo import Inferneo\nimport asyncio\nimport aiohttp\n\nclass AutoFailoverClient:\n    def __init__(self, coordinator_hosts):\n        self.coordinators = coordinator_hosts\n        self.current_coordinator = 0\n        self.session = None\n\n    async def _get_session(self):\n        if self.session is None:\n            self.session = aiohttp.ClientSession()\n        return self.session\n\n    async def generate(self, prompt, **kwargs):\n        session = await self._get_session()\n\n        for attempt in range(len(self.coordinators)):\n            coordinator = self.coordinators[self.current_coordinator]\n\n            try:\n                async with session.post(\n                    f\"http://{coordinator}/v1/completions\",\n                    json={\n                        \"prompt\": prompt,\n                        \"max_tokens\": kwargs.get(\"max_tokens\", 100),\n                        \"temperature\": kwargs.get(\"temperature\", 0.7)\n                    },\n                    timeout=aiohttp.ClientTimeout(total=30)\n                ) as response:\n                    if response.status == 200:\n                        data = await response.json()\n                        return data[\"choices\"][0][\"text\"]\n                    else:\n                        raise Exception(f\"HTTP {response.status}\")\n\n            except Exception as e:\n                print(f\"Failed with {coordinator}: {e}\")\n                # Move to next coordinator\n                self.current_coordinator = (self.current_coordinator + 1) % len(self.coordinators)\n\n        raise Exception(\"All coordinators failed\")\n\n    async def close(self):\n        if self.session:\n            await self.session.close()\n\n# Usage\nasync def main():\n    coordinator_hosts = [\n        \"192.168.1.10:8000\",\n        \"192.168.1.11:8000\",\n        \"192.168.1.12:8000\"\n    ]\n\n    client = AutoFailoverClient(coordinator_hosts)\n\n    try:\n        response = await client.generate(\"Explain automatic failover\")\n        print(response)\n    finally:\n        await client.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"user-guide/distributed-inference/#monitoring-and-metrics","title":"Monitoring and Metrics","text":""},{"location":"user-guide/distributed-inference/#performance-metrics","title":"Performance Metrics","text":"<pre><code>import time\nimport psutil\nfrom collections import defaultdict\nfrom inferneo import Inferneo\n\nclass DistributedMetrics:\n    def __init__(self):\n        self.latency_history = defaultdict(list)\n        self.throughput_history = []\n        self.memory_history = []\n        self.error_count = defaultdict(int)\n\n    def record_request(self, coordinator, latency, success=True):\n        self.latency_history[coordinator].append(latency)\n\n        if not success:\n            self.error_count[coordinator] += 1\n\n    def record_throughput(self, requests_per_second):\n        self.throughput_history.append(requests_per_second)\n\n    def record_memory(self, memory_mb):\n        self.memory_history.append(memory_mb)\n\n    def get_stats(self):\n        stats = {}\n\n        # Latency stats per coordinator\n        for coordinator, latencies in self.latency_history.items():\n            if latencies:\n                stats[f\"{coordinator}_avg_latency\"] = sum(latencies) / len(latencies)\n                stats[f\"{coordinator}_min_latency\"] = min(latencies)\n                stats[f\"{coordinator}_max_latency\"] = max(latencies)\n\n        # Overall stats\n        if self.throughput_history:\n            stats[\"avg_throughput\"] = sum(self.throughput_history) / len(self.throughput_history)\n\n        if self.memory_history:\n            stats[\"avg_memory\"] = sum(self.memory_history) / len(self.memory_history)\n\n        # Error rates\n        total_requests = sum(len(latencies) for latencies in self.latency_history.values())\n        total_errors = sum(self.error_count.values())\n        if total_requests &gt; 0:\n            stats[\"error_rate\"] = total_errors / total_requests\n\n        return stats\n\n# Usage\nmetrics = DistributedMetrics()\nclient = Inferneo(\"http://localhost:8000\")\n\n# Simulate requests and record metrics\nfor i in range(100):\n    start_time = time.time()\n    try:\n        response = client.generate(f\"Test prompt {i}\")\n        latency = time.time() - start_time\n        metrics.record_request(\"coordinator1\", latency, success=True)\n    except Exception as e:\n        latency = time.time() - start_time\n        metrics.record_request(\"coordinator1\", latency, success=False)\n\n    # Record memory usage\n    memory_mb = psutil.virtual_memory().used / (1024 * 1024)\n    metrics.record_memory(memory_mb)\n\nprint(\"Metrics:\", metrics.get_stats())\n</code></pre>"},{"location":"user-guide/distributed-inference/#resource-monitoring","title":"Resource Monitoring","text":"<pre><code>import psutil\nimport GPUtil\nfrom inferneo import Inferneo\n\ndef monitor_distributed_resources():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Monitor system resources\n    cpu_percent = psutil.cpu_percent(interval=1)\n    memory = psutil.virtual_memory()\n\n    # Monitor GPU resources\n    gpu_info = []\n    try:\n        gpus = GPUtil.getGPUs()\n        for gpu in gpus:\n            gpu_info.append({\n                \"id\": gpu.id,\n                \"name\": gpu.name,\n                \"memory_used\": gpu.memoryUsed,\n                \"memory_total\": gpu.memoryTotal,\n                \"memory_percent\": gpu.memoryUtil * 100,\n                \"temperature\": gpu.temperature,\n                \"load\": gpu.load * 100\n            })\n    except:\n        pass\n\n    # Monitor network\n    network = psutil.net_io_counters()\n\n    # Monitor disk\n    disk = psutil.disk_usage('/')\n\n    return {\n        \"cpu_percent\": cpu_percent,\n        \"memory_percent\": memory.percent,\n        \"memory_available_gb\": memory.available / (1024**3),\n        \"gpu_info\": gpu_info,\n        \"network_bytes_sent\": network.bytes_sent,\n        \"network_bytes_recv\": network.bytes_recv,\n        \"disk_percent\": disk.percent,\n        \"disk_free_gb\": disk.free / (1024**3)\n    }\n\n# Continuous monitoring\nimport time\n\ndef continuous_monitoring(duration_seconds=300):\n    start_time = time.time()\n    metrics_history = []\n\n    while time.time() - start_time &lt; duration_seconds:\n        metrics = monitor_distributed_resources()\n        metrics_history.append(metrics)\n\n        print(f\"CPU: {metrics['cpu_percent']:.1f}%\")\n        print(f\"Memory: {metrics['memory_percent']:.1f}%\")\n        print(f\"GPU Memory: {[gpu['memory_percent'] for gpu in metrics['gpu_info']]}\")\n        print(\"-\" * 40)\n\n        time.sleep(10)  # Monitor every 10 seconds\n\n    return metrics_history\n\n# Run monitoring\n# metrics_history = continuous_monitoring(60)  # Monitor for 1 minute\n</code></pre>"},{"location":"user-guide/distributed-inference/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/distributed-inference/#configuration-guidelines","title":"Configuration Guidelines","text":"<pre><code># For high-throughput production\nproduction_config = {\n    \"tensor_parallel_size\": 4,\n    \"pipeline_parallel_size\": 2,\n    \"gpu_memory_utilization\": 0.8,\n    \"max_model_len\": 4096,\n    \"max_batch_size\": 32\n}\n\n# For memory-constrained environments\nmemory_constrained_config = {\n    \"tensor_parallel_size\": 2,\n    \"pipeline_parallel_size\": 1,\n    \"gpu_memory_utilization\": 0.6,\n    \"max_model_len\": 2048,\n    \"max_batch_size\": 16\n}\n\n# For development/testing\ndev_config = {\n    \"tensor_parallel_size\": 1,\n    \"pipeline_parallel_size\": 1,\n    \"gpu_memory_utilization\": 0.7,\n    \"max_model_len\": 1024,\n    \"max_batch_size\": 8\n}\n</code></pre>"},{"location":"user-guide/distributed-inference/#network-optimization","title":"Network Optimization","text":"<pre><code># Optimize network settings for distributed inference\nimport socket\n\ndef optimize_network():\n    # Set socket options for better performance\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n\n    # Set buffer sizes\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1024 * 1024)  # 1MB\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 1024 * 1024)  # 1MB\n\n    return sock\n\n# Configure client with optimized network settings\nclient = Inferneo(\"http://localhost:8000\")\nclient.set_network_config(\n    timeout=30,\n    max_retries=3,\n    connection_pool_size=10\n)\n</code></pre>"},{"location":"user-guide/distributed-inference/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/distributed-inference/#common-issues","title":"Common Issues","text":"<p>Network Connectivity <pre><code># Test network connectivity between nodes\nimport subprocess\n\ndef test_connectivity(hosts):\n    for host in hosts:\n        try:\n            result = subprocess.run(\n                [\"ping\", \"-c\", \"3\", host.split(\":\")[0]], \n                capture_output=True, \n                text=True\n            )\n            if result.returncode == 0:\n                print(f\"\u2713 {host} is reachable\")\n            else:\n                print(f\"\u2717 {host} is not reachable\")\n        except Exception as e:\n            print(f\"\u2717 Error testing {host}: {e}\")\n\n# Test connectivity\nhosts = [\"192.168.1.10:8000\", \"192.168.1.11:8000\", \"192.168.1.12:8000\"]\ntest_connectivity(hosts)\n</code></pre></p> <p>Memory Issues <pre><code># Monitor and resolve memory issues\ndef resolve_memory_issues():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Reduce memory usage\n    client.set_config(\n        gpu_memory_utilization=0.6,  # Reduce GPU memory usage\n        max_model_len=1024,  # Reduce context length\n        max_batch_size=8  # Reduce batch size\n    )\n\n    # Force garbage collection\n    import gc\n    gc.collect()\n\n    # Monitor memory\n    memory = psutil.virtual_memory()\n    if memory.percent &gt; 90:\n        print(\"Warning: High memory usage detected\")\n        return False\n\n    return True\n</code></pre></p> <p>Load Balancing Issues <pre><code># Implement health checks for load balancer\ndef health_check(host):\n    try:\n        response = requests.get(f\"http://{host}/health\", timeout=5)\n        return response.status_code == 200\n    except:\n        return False\n\ndef update_load_balancer(hosts):\n    healthy_hosts = [host for host in hosts if health_check(host)]\n    if not healthy_hosts:\n        raise Exception(\"No healthy hosts available\")\n    return healthy_hosts\n</code></pre></p> <p>For more advanced distributed computing techniques, see the Performance Tuning guide. </p>"},{"location":"user-guide/model-loading/","title":"Model Loading","text":""},{"location":"user-guide/model-loading/#overview","title":"Overview","text":"<p>Guide for loading models in Inferneo.</p>"},{"location":"user-guide/model-loading/#supported-formats","title":"Supported Formats","text":"<ul> <li>Hugging Face Transformers</li> <li>ONNX</li> <li>TorchScript</li> <li>TensorRT</li> </ul>"},{"location":"user-guide/model-loading/#basic-loading","title":"Basic Loading","text":""},{"location":"user-guide/model-loading/#hugging-face-models","title":"Hugging Face Models","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"user-guide/model-loading/#local-models","title":"Local Models","text":"<pre><code>inferneo serve --model /path/to/local/model\n</code></pre>"},{"location":"user-guide/model-loading/#configuration","title":"Configuration","text":""},{"location":"user-guide/model-loading/#memory-settings","title":"Memory Settings","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf --gpu-memory-utilization 0.8\n</code></pre>"},{"location":"user-guide/model-loading/#multiple-models","title":"Multiple Models","text":"<pre><code>inferneo serve --model model1 --model model2\n</code></pre>"},{"location":"user-guide/model-loading/#next-steps","title":"Next Steps","text":"<ul> <li>Batching</li> <li>Streaming</li> <li>Quantization</li> <li>Distributed Inference </li> </ul>"},{"location":"user-guide/offline-inference/","title":"Offline Inference","text":"<p>This guide covers how to use Inferneo for offline inference tasks.</p>"},{"location":"user-guide/offline-inference/#overview","title":"Overview","text":"<p>Offline inference allows you to process batches of data without real-time constraints, making it ideal for:</p> <ul> <li>Batch processing of large datasets</li> <li>Model evaluation and testing</li> <li>Data preprocessing and analysis</li> <li>Research and development workflows</li> </ul>"},{"location":"user-guide/offline-inference/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/offline-inference/#single-file-processing","title":"Single File Processing","text":"<pre><code>from inferneo import InferneoClient\n\nclient = InferneoClient(\"http://localhost:8000\")\n\n# Process a single file\nwith open(\"input.txt\", \"r\") as f:\n    content = f.read()\n\nresponse = client.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    prompt=content,\n    max_tokens=200\n)\n\nwith open(\"output.txt\", \"w\") as f:\n    f.write(response.choices[0].text)\n</code></pre>"},{"location":"user-guide/offline-inference/#batch-file-processing","title":"Batch File Processing","text":"<pre><code>import os\nfrom pathlib import Path\n\ndef process_directory(input_dir, output_dir, model_id):\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    for file_path in input_path.glob(\"*.txt\"):\n        with open(file_path, \"r\") as f:\n            content = f.read()\n\n        response = client.completions.create(\n            model=model_id,\n            prompt=content,\n            max_tokens=200\n        )\n\n        output_file = output_path / f\"{file_path.stem}_processed.txt\"\n        with open(output_file, \"w\") as f:\n            f.write(response.choices[0].text)\n\n# Usage\nprocess_directory(\"input_files\", \"output_files\", \"meta-llama/Llama-2-7b-chat-hf\")\n</code></pre>"},{"location":"user-guide/offline-inference/#advanced-batch-processing","title":"Advanced Batch Processing","text":""},{"location":"user-guide/offline-inference/#parallel-processing","title":"Parallel Processing","text":"<pre><code>import concurrent.futures\nfrom typing import List, Dict\n\ndef process_batch(prompts: List[str], model_id: str) -&gt; List[Dict]:\n    \"\"\"Process a batch of prompts in parallel.\"\"\"\n\n    def process_single(prompt):\n        try:\n            response = client.completions.create(\n                model=model_id,\n                prompt=prompt,\n                max_tokens=100\n            )\n            return {\n                \"prompt\": prompt,\n                \"response\": response.choices[0].text,\n                \"status\": \"success\"\n            }\n        except Exception as e:\n            return {\n                \"prompt\": prompt,\n                \"response\": None,\n                \"status\": \"error\",\n                \"error\": str(e)\n            }\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        results = list(executor.map(process_single, prompts))\n\n    return results\n\n# Usage\nprompts = [\n    \"Explain machine learning\",\n    \"What is deep learning?\",\n    \"Describe neural networks\"\n]\n\nresults = process_batch(prompts, \"meta-llama/Llama-2-7b-chat-hf\")\n</code></pre>"},{"location":"user-guide/offline-inference/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code>def process_large_dataset(file_path: str, batch_size: int = 100):\n    \"\"\"Process large datasets in memory-efficient batches.\"\"\"\n\n    results = []\n\n    with open(file_path, \"r\") as f:\n        batch = []\n        for line in f:\n            batch.append(line.strip())\n\n            if len(batch) &gt;= batch_size:\n                # Process batch\n                batch_results = process_batch(batch, \"meta-llama/Llama-2-7b-chat-hf\")\n                results.extend(batch_results)\n                batch = []\n\n        # Process remaining items\n        if batch:\n            batch_results = process_batch(batch, \"meta-llama/Llama-2-7b-chat-hf\")\n            results.extend(batch_results)\n\n    return results\n</code></pre>"},{"location":"user-guide/offline-inference/#data-formats","title":"Data Formats","text":""},{"location":"user-guide/offline-inference/#csv-processing","title":"CSV Processing","text":"<pre><code>import pandas as pd\n\ndef process_csv(input_file: str, output_file: str, prompt_column: str):\n    \"\"\"Process CSV files with prompts.\"\"\"\n\n    # Read input CSV\n    df = pd.read_csv(input_file)\n\n    # Process each row\n    responses = []\n    for _, row in df.iterrows():\n        prompt = row[prompt_column]\n        response = client.completions.create(\n            model=\"meta-llama/Llama-2-7b-chat-hf\",\n            prompt=prompt,\n            max_tokens=100\n        )\n        responses.append(response.choices[0].text)\n\n    # Add responses to dataframe\n    df['response'] = responses\n\n    # Save output\n    df.to_csv(output_file, index=False)\n    return df\n\n# Usage\nprocess_csv(\"input.csv\", \"output.csv\", \"question\")\n</code></pre>"},{"location":"user-guide/offline-inference/#json-processing","title":"JSON Processing","text":"<pre><code>import json\n\ndef process_json_file(input_file: str, output_file: str):\n    \"\"\"Process JSON files with structured data.\"\"\"\n\n    with open(input_file, \"r\") as f:\n        data = json.load(f)\n\n    processed_data = []\n    for item in data:\n        prompt = item.get(\"prompt\", \"\")\n        response = client.completions.create(\n            model=\"meta-llama/Llama-2-7b-chat-hf\",\n            prompt=prompt,\n            max_tokens=100\n        )\n\n        processed_item = {\n            **item,\n            \"response\": response.choices[0].text,\n            \"model\": \"meta-llama/Llama-2-7b-chat-hf\"\n        }\n        processed_data.append(processed_item)\n\n    with open(output_file, \"w\") as f:\n        json.dump(processed_data, f, indent=2)\n\n    return processed_data\n</code></pre>"},{"location":"user-guide/offline-inference/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/offline-inference/#batch-size-tuning","title":"Batch Size Tuning","text":"<pre><code>def find_optimal_batch_size(test_prompts: List[str], model_id: str):\n    \"\"\"Find the optimal batch size for your hardware.\"\"\"\n\n    batch_sizes = [1, 4, 8, 16, 32]\n    results = {}\n\n    for batch_size in batch_sizes:\n        start_time = time.time()\n\n        # Process test batch\n        process_batch(test_prompts[:batch_size], model_id)\n\n        end_time = time.time()\n        throughput = batch_size / (end_time - start_time)\n\n        results[batch_size] = {\n            \"time\": end_time - start_time,\n            \"throughput\": throughput\n        }\n\n    return results\n\n# Usage\ntest_prompts = [\"Test prompt\"] * 32\noptimal_batch = find_optimal_batch_size(test_prompts, \"meta-llama/Llama-2-7b-chat-hf\")\nprint(f\"Optimal batch size: {max(optimal_batch, key=lambda x: optimal_batch[x]['throughput'])}\")\n</code></pre>"},{"location":"user-guide/offline-inference/#memory-management","title":"Memory Management","text":"<pre><code>import gc\n\ndef process_with_memory_management(prompts: List[str], model_id: str):\n    \"\"\"Process with explicit memory management.\"\"\"\n\n    results = []\n    batch_size = 16\n\n    for i in range(0, len(prompts), batch_size):\n        batch = prompts[i:i + batch_size]\n\n        # Process batch\n        batch_results = process_batch(batch, model_id)\n        results.extend(batch_results)\n\n        # Clear memory\n        gc.collect()\n\n        # Optional: Add delay to prevent overwhelming the server\n        time.sleep(0.1)\n\n    return results\n</code></pre>"},{"location":"user-guide/offline-inference/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/offline-inference/#robust-processing","title":"Robust Processing","text":"<pre><code>def robust_batch_processing(prompts: List[str], model_id: str, max_retries: int = 3):\n    \"\"\"Process with robust error handling and retries.\"\"\"\n\n    results = []\n\n    for i, prompt in enumerate(prompts):\n        for attempt in range(max_retries):\n            try:\n                response = client.completions.create(\n                    model=model_id,\n                    prompt=prompt,\n                    max_tokens=100\n                )\n\n                results.append({\n                    \"index\": i,\n                    \"prompt\": prompt,\n                    \"response\": response.choices[0].text,\n                    \"status\": \"success\",\n                    \"attempts\": attempt + 1\n                })\n                break\n\n            except Exception as e:\n                if attempt == max_retries - 1:\n                    results.append({\n                        \"index\": i,\n                        \"prompt\": prompt,\n                        \"response\": None,\n                        \"status\": \"failed\",\n                        \"error\": str(e),\n                        \"attempts\": max_retries\n                    })\n                else:\n                    time.sleep(2 ** attempt)  # Exponential backoff\n\n    return results\n</code></pre>"},{"location":"user-guide/offline-inference/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"user-guide/offline-inference/#progress-tracking","title":"Progress Tracking","text":"<pre><code>from tqdm import tqdm\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef process_with_progress(prompts: List[str], model_id: str):\n    \"\"\"Process with progress tracking and logging.\"\"\"\n\n    results = []\n\n    for i, prompt in tqdm(enumerate(prompts), total=len(prompts)):\n        try:\n            response = client.completions.create(\n                model=model_id,\n                prompt=prompt,\n                max_tokens=100\n            )\n\n            results.append({\n                \"index\": i,\n                \"prompt\": prompt,\n                \"response\": response.choices[0].text\n            })\n\n            if i % 100 == 0:\n                logger.info(f\"Processed {i}/{len(prompts)} prompts\")\n\n        except Exception as e:\n            logger.error(f\"Error processing prompt {i}: {e}\")\n            results.append({\n                \"index\": i,\n                \"prompt\": prompt,\n                \"response\": None,\n                \"error\": str(e)\n            })\n\n    return results\n</code></pre>"},{"location":"user-guide/offline-inference/#best-practices","title":"Best Practices","text":"<ol> <li>Batch Processing: Group requests to maximize throughput</li> <li>Error Handling: Implement retries and graceful error handling</li> <li>Memory Management: Process large datasets in chunks</li> <li>Progress Tracking: Monitor progress for long-running tasks</li> <li>Resource Management: Clean up resources and manage connections</li> <li>Logging: Log important events and errors for debugging</li> </ol>"},{"location":"user-guide/offline-inference/#next-steps","title":"Next Steps","text":"<ul> <li>Online Serving - Learn about real-time inference</li> <li>Batching - Optimize batch processing performance</li> <li>Performance Tuning - Advanced optimization techniques </li> </ul>"},{"location":"user-guide/online-serving/","title":"Online Serving","text":"<p>This guide covers how to deploy and use Inferneo for online serving with real-time inference capabilities.</p>"},{"location":"user-guide/online-serving/#overview","title":"Overview","text":"<p>Online serving enables real-time inference for applications that require immediate responses, such as:</p> <ul> <li>Chat applications and conversational AI</li> <li>Web services and APIs</li> <li>Real-time decision making systems</li> <li>Interactive applications and demos</li> </ul>"},{"location":"user-guide/online-serving/#server-setup","title":"Server Setup","text":""},{"location":"user-guide/online-serving/#starting-the-server","title":"Starting the Server","text":"<pre><code># Basic server startup\ninferneo serve --model meta-llama/Llama-2-7b-chat-hf\n\n# With custom configuration\ninferneo serve \\\n    --model meta-llama/Llama-2-7b-chat-hf \\\n    --host 0.0.0.0 \\\n    --port 8000 \\\n    --max-model-len 4096 \\\n    --gpu-memory-utilization 0.9\n</code></pre>"},{"location":"user-guide/online-serving/#configuration-options","title":"Configuration Options","text":"<pre><code># config.yaml\nmodel: meta-llama/Llama-2-7b-chat-hf\nhost: 0.0.0.0\nport: 8000\nmax_model_len: 4096\ngpu_memory_utilization: 0.9\ntensor_parallel_size: 1\nmax_num_batched_tokens: 4096\nmax_num_seqs: 256\n</code></pre> <pre><code># Using configuration file\ninferneo serve --config config.yaml\n</code></pre>"},{"location":"user-guide/online-serving/#client-usage","title":"Client Usage","text":""},{"location":"user-guide/online-serving/#python-client","title":"Python Client","text":"<pre><code>from inferneo import InferneoClient\n\n# Initialize client\nclient = InferneoClient(\"http://localhost:8000\")\n\n# Basic completion\nresponse = client.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    prompt=\"Explain quantum computing\",\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(response.choices[0].text)\n</code></pre>"},{"location":"user-guide/online-serving/#chat-completions","title":"Chat Completions","text":"<pre><code># Chat completion with conversation history\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n    {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"},\n    {\"role\": \"user\", \"content\": \"Can you give me an example?\"}\n]\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    messages=messages,\n    max_tokens=150,\n    temperature=0.7\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"user-guide/online-serving/#streaming-responses","title":"Streaming Responses","text":"<pre><code># Streaming for real-time responses\nstream = client.chat.completions.create(\n    model=\"meta-llama/Llama-2-7b-chat-hf\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short story about a robot.\"}],\n    max_tokens=200,\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"user-guide/online-serving/#rest-api","title":"REST API","text":""},{"location":"user-guide/online-serving/#basic-endpoints","title":"Basic Endpoints","text":"<pre><code># Health check\ncurl http://localhost:8000/health\n\n# List models\ncurl http://localhost:8000/v1/models\n\n# Text completion\ncurl -X POST http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"prompt\": \"Explain artificial intelligence\",\n    \"max_tokens\": 100,\n    \"temperature\": 0.7\n  }'\n\n# Chat completion\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ],\n    \"max_tokens\": 100\n  }'\n</code></pre>"},{"location":"user-guide/online-serving/#advanced-api-usage","title":"Advanced API Usage","text":"<pre><code>import requests\nimport json\n\n# Function to make API calls\ndef inferneo_api_call(prompt, model=\"meta-llama/Llama-2-7b-chat-hf\"):\n    url = \"http://localhost:8000/v1/completions\"\n    headers = {\"Content-Type\": \"application/json\"}\n\n    data = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"max_tokens\": 100,\n        \"temperature\": 0.7,\n        \"top_p\": 0.9,\n        \"frequency_penalty\": 0.0,\n        \"presence_penalty\": 0.0\n    }\n\n    response = requests.post(url, headers=headers, json=data)\n    return response.json()\n\n# Usage\nresult = inferneo_api_call(\"Explain quantum computing\")\nprint(result[\"choices\"][0][\"text\"])\n</code></pre>"},{"location":"user-guide/online-serving/#websocket-api","title":"WebSocket API","text":""},{"location":"user-guide/online-serving/#real-time-communication","title":"Real-time Communication","text":"<pre><code>import asyncio\nimport websockets\nimport json\n\nasync def chat_with_websocket():\n    uri = \"ws://localhost:8000/v1/chat/completions\"\n\n    async with websockets.connect(uri) as websocket:\n        # Send message\n        message = {\n            \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n            \"stream\": True\n        }\n\n        await websocket.send(json.dumps(message))\n\n        # Receive streaming response\n        async for response in websocket:\n            data = json.loads(response)\n            if \"choices\" in data and len(data[\"choices\"]) &gt; 0:\n                delta = data[\"choices\"][0].get(\"delta\", {})\n                if \"content\" in delta:\n                    print(delta[\"content\"], end=\"\", flush=True)\n\n# Run the async function\nasyncio.run(chat_with_websocket())\n</code></pre>"},{"location":"user-guide/online-serving/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/online-serving/#request-batching","title":"Request Batching","text":"<pre><code># Batch multiple requests for better throughput\ndef batch_requests(prompts, model_id):\n    batch_data = {\n        \"model\": model_id,\n        \"prompts\": prompts,\n        \"max_tokens\": 100,\n        \"temperature\": 0.7\n    }\n\n    response = requests.post(\n        \"http://localhost:8000/v1/completions/batch\",\n        json=batch_data\n    )\n\n    return response.json()\n\n# Usage\nprompts = [\n    \"Explain machine learning\",\n    \"What is deep learning?\",\n    \"Describe neural networks\"\n]\n\nresults = batch_requests(prompts, \"meta-llama/Llama-2-7b-chat-hf\")\n</code></pre>"},{"location":"user-guide/online-serving/#connection-pooling","title":"Connection Pooling","text":"<pre><code>import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\n# Create session with connection pooling\nsession = requests.Session()\n\n# Configure retry strategy\nretry_strategy = Retry(\n    total=3,\n    backoff_factor=1,\n    status_forcelist=[429, 500, 502, 503, 504],\n)\n\nadapter = HTTPAdapter(max_retries=retry_strategy)\nsession.mount(\"http://\", adapter)\nsession.mount(\"https://\", adapter)\n\n# Use session for multiple requests\ndef make_request(prompt):\n    data = {\n        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"prompt\": prompt,\n        \"max_tokens\": 100\n    }\n\n    response = session.post(\n        \"http://localhost:8000/v1/completions\",\n        json=data\n    )\n    return response.json()\n</code></pre>"},{"location":"user-guide/online-serving/#load-balancing","title":"Load Balancing","text":""},{"location":"user-guide/online-serving/#multiple-server-instances","title":"Multiple Server Instances","text":"<pre><code>import random\nfrom typing import List\n\nclass LoadBalancedClient:\n    def __init__(self, servers: List[str]):\n        self.servers = servers\n        self.current_server = 0\n\n    def get_next_server(self):\n        # Round-robin load balancing\n        server = self.servers[self.current_server]\n        self.current_server = (self.current_server + 1) % len(self.servers)\n        return server\n\n    def request(self, prompt):\n        server = self.get_next_server()\n        url = f\"{server}/v1/completions\"\n\n        data = {\n            \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n            \"prompt\": prompt,\n            \"max_tokens\": 100\n        }\n\n        response = requests.post(url, json=data)\n        return response.json()\n\n# Usage\nservers = [\n    \"http://server1:8000\",\n    \"http://server2:8000\",\n    \"http://server3:8000\"\n]\n\nclient = LoadBalancedClient(servers)\nresult = client.request(\"Explain AI\")\n</code></pre>"},{"location":"user-guide/online-serving/#monitoring-and-health-checks","title":"Monitoring and Health Checks","text":""},{"location":"user-guide/online-serving/#health-monitoring","title":"Health Monitoring","text":"<pre><code>import time\nimport requests\n\ndef monitor_server_health(server_url, interval=30):\n    \"\"\"Monitor server health and performance.\"\"\"\n\n    while True:\n        try:\n            # Health check\n            health_response = requests.get(f\"{server_url}/health\")\n            health_status = health_response.json()\n\n            # Performance metrics\n            metrics_response = requests.get(f\"{server_url}/metrics\")\n            metrics = metrics_response.json()\n\n            print(f\"Health: {health_status['status']}\")\n            print(f\"Active requests: {metrics.get('active_requests', 0)}\")\n            print(f\"Queue size: {metrics.get('queue_size', 0)}\")\n            print(f\"GPU utilization: {metrics.get('gpu_utilization', 0)}%\")\n\n        except Exception as e:\n            print(f\"Error monitoring server: {e}\")\n\n        time.sleep(interval)\n\n# Usage\nmonitor_server_health(\"http://localhost:8000\")\n</code></pre>"},{"location":"user-guide/online-serving/#error-handling","title":"Error Handling","text":"<pre><code>def robust_request(prompt, max_retries=3):\n    \"\"\"Make requests with robust error handling.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            response = requests.post(\n                \"http://localhost:8000/v1/completions\",\n                json={\n                    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n                    \"prompt\": prompt,\n                    \"max_tokens\": 100\n                },\n                timeout=30\n            )\n\n            response.raise_for_status()\n            return response.json()\n\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                raise e\n            else:\n                time.sleep(2 ** attempt)  # Exponential backoff\n</code></pre>"},{"location":"user-guide/online-serving/#security-considerations","title":"Security Considerations","text":""},{"location":"user-guide/online-serving/#authentication","title":"Authentication","text":"<pre><code># API key authentication\nheaders = {\n    \"Authorization\": \"Bearer your-api-key\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(\n    \"http://localhost:8000/v1/completions\",\n    headers=headers,\n    json={\n        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"prompt\": \"Hello\",\n        \"max_tokens\": 100\n    }\n)\n</code></pre>"},{"location":"user-guide/online-serving/#rate-limiting","title":"Rate Limiting","text":"<pre><code>import time\nfrom collections import deque\n\nclass RateLimiter:\n    def __init__(self, max_requests, time_window):\n        self.max_requests = max_requests\n        self.time_window = time_window\n        self.requests = deque()\n\n    def can_make_request(self):\n        now = time.time()\n\n        # Remove old requests\n        while self.requests and self.requests[0] &lt; now - self.time_window:\n            self.requests.popleft()\n\n        if len(self.requests) &lt; self.max_requests:\n            self.requests.append(now)\n            return True\n\n        return False\n\n# Usage\nrate_limiter = RateLimiter(max_requests=10, time_window=60)  # 10 requests per minute\n\ndef rate_limited_request(prompt):\n    if rate_limiter.can_make_request():\n        return make_request(prompt)\n    else:\n        raise Exception(\"Rate limit exceeded\")\n</code></pre>"},{"location":"user-guide/online-serving/#deployment-examples","title":"Deployment Examples","text":""},{"location":"user-guide/online-serving/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Dockerfile\nFROM python:3.9-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8000\nCMD [\"inferneo\", \"serve\", \"--model\", \"meta-llama/Llama-2-7b-chat-hf\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre> <pre><code># Build and run\ndocker build -t inferneo-server .\ndocker run -p 8000:8000 --gpus all inferneo-server\n</code></pre>"},{"location":"user-guide/online-serving/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inferneo-server\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: inferneo-server\n  template:\n    metadata:\n      labels:\n        app: inferneo-server\n    spec:\n      containers:\n      - name: inferneo\n        image: inferneo-server:latest\n        ports:\n        - containerPort: 8000\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n          requests:\n            memory: \"8Gi\"\n            cpu: \"4\"\n</code></pre>"},{"location":"user-guide/online-serving/#best-practices","title":"Best Practices","text":"<ol> <li>Connection Management: Use connection pooling for multiple requests</li> <li>Error Handling: Implement retries and graceful error handling</li> <li>Monitoring: Monitor server health and performance metrics</li> <li>Load Balancing: Distribute load across multiple server instances</li> <li>Security: Implement authentication and rate limiting</li> <li>Caching: Cache frequently requested responses when appropriate</li> </ol>"},{"location":"user-guide/online-serving/#next-steps","title":"Next Steps","text":"<ul> <li>Offline Inference - Learn about batch processing</li> <li>Batching - Optimize request batching</li> <li>Performance Tuning - Advanced optimization techniques </li> </ul>"},{"location":"user-guide/quantization/","title":"Quantization","text":"<p>Quantization is a technique that reduces the memory footprint and computational requirements of large language models by using lower precision data types.</p>"},{"location":"user-guide/quantization/#overview","title":"Overview","text":"<p>Quantization converts model weights from high precision (typically FP16 or FP32) to lower precision formats (INT8, INT4, etc.), significantly reducing memory usage while maintaining reasonable accuracy.</p>"},{"location":"user-guide/quantization/#supported-quantization-methods","title":"Supported Quantization Methods","text":""},{"location":"user-guide/quantization/#awq-activation-aware-weight-quantization","title":"AWQ (Activation-aware Weight Quantization)","text":"<p>AWQ is an efficient quantization method that considers activation statistics during quantization:</p> <pre><code>from inferneo import Inferneo\n\n# Load model with AWQ quantization\nclient = Inferneo(\"http://localhost:8000\")\n\n# Use AWQ quantized model\nresponse = client.generate(\n    \"Explain quantum computing\",\n    model=\"meta-llama/Llama-2-7b-chat-hf-awq\"\n)\n</code></pre>"},{"location":"user-guide/quantization/#gptq-gradient-based-post-training-quantization","title":"GPTQ (Gradient-based Post-training Quantization)","text":"<p>GPTQ provides high-quality quantization with minimal accuracy loss:</p> <pre><code>from inferneo import Inferneo\n\n# Load GPTQ quantized model\nclient = Inferneo(\"http://localhost:8000\")\n\n# Use GPTQ quantized model\nresponse = client.generate(\n    \"Write a story about AI\",\n    model=\"meta-llama/Llama-2-7b-chat-hf-gptq\"\n)\n</code></pre>"},{"location":"user-guide/quantization/#squeezellm","title":"SqueezeLLM","text":"<p>SqueezeLLM offers efficient quantization with sparsity:</p> <pre><code>from inferneo import Inferneo\n\n# Load SqueezeLLM quantized model\nclient = Inferneo(\"http://localhost:8000\")\n\n# Use SqueezeLLM quantized model\nresponse = client.generate(\n    \"Explain machine learning\",\n    model=\"meta-llama/Llama-2-7b-chat-hf-squeezellm\"\n)\n</code></pre>"},{"location":"user-guide/quantization/#quantization-configuration","title":"Quantization Configuration","text":""},{"location":"user-guide/quantization/#server-configuration","title":"Server Configuration","text":"<p>Configure quantization settings in your server configuration:</p> <pre><code># config.yaml\nmodel:\n  name: \"meta-llama/Llama-2-7b-chat-hf\"\n  quantization: \"awq\"\n  quantization_config:\n    bits: 4\n    group_size: 128\n    zero_point: true\n    scale: true\n</code></pre>"},{"location":"user-guide/quantization/#dynamic-quantization","title":"Dynamic Quantization","text":"<p>Apply quantization dynamically at runtime:</p> <pre><code>from inferneo import Inferneo\n\nclient = Inferneo(\"http://localhost:8000\")\n\n# Configure quantization parameters\nclient.set_quantization_config(\n    method=\"awq\",\n    bits=4,\n    group_size=128,\n    zero_point=True,\n    scale=True\n)\n\n# Generate with quantization\nresponse = client.generate(\"Explain neural networks\")\n</code></pre>"},{"location":"user-guide/quantization/#performance-comparison","title":"Performance Comparison","text":""},{"location":"user-guide/quantization/#memory-usage-comparison","title":"Memory Usage Comparison","text":"<pre><code>import psutil\nimport time\nfrom inferneo import Inferneo\n\ndef benchmark_memory_usage():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Test different quantization methods\n    models = [\n        \"meta-llama/Llama-2-7b-chat-hf\",  # No quantization\n        \"meta-llama/Llama-2-7b-chat-hf-awq\",  # AWQ\n        \"meta-llama/Llama-2-7b-chat-hf-gptq\",  # GPTQ\n        \"meta-llama/Llama-2-7b-chat-hf-squeezellm\"  # SqueezeLLM\n    ]\n\n    results = {}\n\n    for model in models:\n        print(f\"Testing {model}...\")\n\n        # Measure memory before\n        memory_before = psutil.virtual_memory().used\n\n        # Load model\n        start_time = time.time()\n        response = client.generate(\"Test prompt\", model=model)\n        load_time = time.time() - start_time\n\n        # Measure memory after\n        memory_after = psutil.virtual_memory().used\n        memory_used = memory_after - memory_before\n\n        results[model] = {\n            \"memory_mb\": memory_used / (1024 * 1024),\n            \"load_time\": load_time\n        }\n\n    # Print results\n    print(\"\\nMemory Usage Comparison:\")\n    print(\"-\" * 60)\n    for model, metrics in results.items():\n        print(f\"{model}: {metrics['memory_mb']:.1f} MB, {metrics['load_time']:.2f}s\")\n\n    return results\n\n# Run benchmark\nbenchmark_memory_usage()\n</code></pre>"},{"location":"user-guide/quantization/#speed-vs-accuracy-trade-off","title":"Speed vs Accuracy Trade-off","text":"<pre><code>from inferneo import Inferneo\nimport time\nimport numpy as np\n\ndef benchmark_speed_accuracy():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Test prompts\n    test_prompts = [\n        \"Explain quantum computing\",\n        \"What is machine learning?\",\n        \"Describe neural networks\",\n        \"How does backpropagation work?\",\n        \"Explain the transformer architecture\"\n    ]\n\n    models = [\n        (\"FP16\", \"meta-llama/Llama-2-7b-chat-hf\"),\n        (\"AWQ\", \"meta-llama/Llama-2-7b-chat-hf-awq\"),\n        (\"GPTQ\", \"meta-llama/Llama-2-7b-chat-hf-gptq\")\n    ]\n\n    results = {}\n\n    for model_name, model_path in models:\n        print(f\"Testing {model_name}...\")\n\n        times = []\n        responses = []\n\n        for prompt in test_prompts:\n            start_time = time.time()\n            response = client.generate(prompt, model=model_path)\n            end_time = time.time()\n\n            times.append(end_time - start_time)\n            responses.append(response.generated_text)\n\n        results[model_name] = {\n            \"avg_time\": np.mean(times),\n            \"std_time\": np.std(times),\n            \"responses\": responses\n        }\n\n    # Print results\n    print(\"\\nSpeed Comparison:\")\n    print(\"-\" * 40)\n    for model_name, metrics in results.items():\n        print(f\"{model_name}: {metrics['avg_time']:.3f}s \u00b1 {metrics['std_time']:.3f}s\")\n\n    return results\n\n# Run benchmark\nbenchmark_speed_accuracy()\n</code></pre>"},{"location":"user-guide/quantization/#custom-quantization","title":"Custom Quantization","text":""},{"location":"user-guide/quantization/#quantizing-your-own-model","title":"Quantizing Your Own Model","text":"<pre><code>from inferneo import Inferneo\nimport torch\n\ndef quantize_model(model_path, output_path, method=\"awq\"):\n    \"\"\"\n    Quantize a model using the specified method.\n    \"\"\"\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Load the original model\n    print(f\"Loading model from {model_path}...\")\n\n    # Configure quantization\n    quantization_config = {\n        \"method\": method,\n        \"bits\": 4,\n        \"group_size\": 128,\n        \"zero_point\": True,\n        \"scale\": True\n    }\n\n    # Apply quantization\n    print(f\"Applying {method} quantization...\")\n    client.quantize_model(\n        model_path=model_path,\n        output_path=output_path,\n        config=quantization_config\n    )\n\n    print(f\"Quantized model saved to {output_path}\")\n\n# Usage\nquantize_model(\n    model_path=\"./my-model\",\n    output_path=\"./my-model-awq\",\n    method=\"awq\"\n)\n</code></pre>"},{"location":"user-guide/quantization/#quantization-with-custom-parameters","title":"Quantization with Custom Parameters","text":"<pre><code>from inferneo import Inferneo\n\ndef custom_quantization():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Custom AWQ configuration\n    awq_config = {\n        \"method\": \"awq\",\n        \"bits\": 4,\n        \"group_size\": 64,  # Smaller group size\n        \"zero_point\": True,\n        \"scale\": True,\n        \"act_order\": True,  # Activation ordering\n        \"true_sequential\": True\n    }\n\n    # Custom GPTQ configuration\n    gptq_config = {\n        \"method\": \"gptq\",\n        \"bits\": 4,\n        \"group_size\": 128,\n        \"desc_act\": True,\n        \"static_groups\": False,\n        \"sym\": False,  # Asymmetric quantization\n        \"true_sequential\": True\n    }\n\n    # Apply custom quantization\n    client.set_quantization_config(**awq_config)\n\n    # Test generation\n    response = client.generate(\"Test prompt with custom quantization\")\n    print(response.generated_text)\n\ncustom_quantization()\n</code></pre>"},{"location":"user-guide/quantization/#memory-optimization","title":"Memory Optimization","text":""},{"location":"user-guide/quantization/#memory-efficient-loading","title":"Memory-Efficient Loading","text":"<pre><code>from inferneo import Inferneo\nimport gc\n\ndef memory_efficient_loading():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Configure for memory efficiency\n    client.set_config(\n        max_model_len=2048,  # Reduce context length\n        gpu_memory_utilization=0.8,  # Limit GPU memory usage\n        swap_space=4  # Enable swap space\n    )\n\n    # Load quantized model\n    response = client.generate(\n        \"Explain the benefits of quantization\",\n        model=\"meta-llama/Llama-2-7b-chat-hf-awq\"\n    )\n\n    # Force garbage collection\n    gc.collect()\n\n    return response\n\n# Usage\nresponse = memory_efficient_loading()\n</code></pre>"},{"location":"user-guide/quantization/#multi-gpu-quantization","title":"Multi-GPU Quantization","text":"<pre><code>from inferneo import Inferneo\n\ndef multi_gpu_quantization():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Configure for multi-GPU\n    client.set_config(\n        tensor_parallel_size=2,  # Use 2 GPUs\n        gpu_memory_utilization=0.7,  # Conservative memory usage\n        max_model_len=4096\n    )\n\n    # Load large quantized model\n    response = client.generate(\n        \"Write a comprehensive guide to AI\",\n        model=\"meta-llama/Llama-2-70b-chat-hf-awq\"\n    )\n\n    return response\n\n# Usage\nresponse = multi_gpu_quantization()\n</code></pre>"},{"location":"user-guide/quantization/#accuracy-evaluation","title":"Accuracy Evaluation","text":""},{"location":"user-guide/quantization/#quantization-impact-assessment","title":"Quantization Impact Assessment","text":"<pre><code>from inferneo import Inferneo\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef evaluate_quantization_impact():\n    client = Inferneo(\"http://localhost:8000\")\n\n    # Test prompts for evaluation\n    evaluation_prompts = [\n        \"Explain the concept of machine learning\",\n        \"What are the advantages of deep learning?\",\n        \"Describe the transformer architecture\",\n        \"How does attention mechanism work?\",\n        \"Explain backpropagation in neural networks\"\n    ]\n\n    # Get responses from different quantization methods\n    responses = {}\n\n    models = [\n        (\"FP16\", \"meta-llama/Llama-2-7b-chat-hf\"),\n        (\"AWQ\", \"meta-llama/Llama-2-7b-chat-hf-awq\"),\n        (\"GPTQ\", \"meta-llama/Llama-2-7b-chat-hf-gptq\")\n    ]\n\n    for model_name, model_path in models:\n        responses[model_name] = []\n\n        for prompt in evaluation_prompts:\n            response = client.generate(prompt, model=model_path)\n            responses[model_name].append(response.generated_text)\n\n    # Calculate similarity scores\n    fp16_responses = responses[\"FP16\"]\n\n    print(\"Quantization Impact Analysis:\")\n    print(\"-\" * 40)\n\n    for model_name in [\"AWQ\", \"GPTQ\"]:\n        similarities = []\n\n        for i in range(len(evaluation_prompts)):\n            # Simple similarity based on response length and content\n            fp16_len = len(fp16_responses[i])\n            quantized_len = len(responses[model_name][i])\n\n            # Length similarity\n            length_sim = min(fp16_len, quantized_len) / max(fp16_len, quantized_len)\n\n            # Content similarity (simple word overlap)\n            fp16_words = set(fp16_responses[i].lower().split())\n            quantized_words = set(responses[model_name][i].lower().split())\n\n            if fp16_words and quantized_words:\n                content_sim = len(fp16_words &amp; quantized_words) / len(fp16_words | quantized_words)\n            else:\n                content_sim = 0\n\n            # Combined similarity\n            combined_sim = (length_sim + content_sim) / 2\n            similarities.append(combined_sim)\n\n        avg_similarity = np.mean(similarities)\n        print(f\"{model_name} vs FP16: {avg_similarity:.3f} similarity\")\n\n    return responses\n\n# Run evaluation\nevaluation_results = evaluate_quantization_impact()\n</code></pre>"},{"location":"user-guide/quantization/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/quantization/#choosing-the-right-quantization-method","title":"Choosing the Right Quantization Method","text":"<ol> <li>AWQ: Best for general use cases with good accuracy/speed balance</li> <li>GPTQ: Best for maximum accuracy preservation</li> <li>SqueezeLLM: Best for memory-constrained environments</li> </ol>"},{"location":"user-guide/quantization/#configuration-guidelines","title":"Configuration Guidelines","text":"<pre><code># For production use\nproduction_config = {\n    \"method\": \"awq\",\n    \"bits\": 4,\n    \"group_size\": 128,\n    \"zero_point\": True,\n    \"scale\": True\n}\n\n# For development/testing\ndev_config = {\n    \"method\": \"gptq\",\n    \"bits\": 4,\n    \"group_size\": 128,\n    \"desc_act\": True\n}\n\n# For memory-constrained environments\nmemory_constrained_config = {\n    \"method\": \"squeezellm\",\n    \"bits\": 4,\n    \"group_size\": 64\n}\n</code></pre>"},{"location":"user-guide/quantization/#monitoring-quantization-performance","title":"Monitoring Quantization Performance","text":"<pre><code>import psutil\nimport time\nfrom inferneo import Inferneo\n\nclass QuantizationMonitor:\n    def __init__(self):\n        self.memory_history = []\n        self.latency_history = []\n\n    def monitor_performance(self, client, prompt, model):\n        # Monitor memory\n        memory_before = psutil.virtual_memory().used\n\n        # Measure latency\n        start_time = time.time()\n        response = client.generate(prompt, model=model)\n        latency = time.time() - start_time\n\n        # Monitor memory after\n        memory_after = psutil.virtual_memory().used\n        memory_used = memory_after - memory_before\n\n        # Record metrics\n        self.memory_history.append(memory_used / (1024 * 1024))  # MB\n        self.latency_history.append(latency)\n\n        return {\n            \"memory_mb\": memory_used / (1024 * 1024),\n            \"latency_s\": latency,\n            \"response_length\": len(response.generated_text)\n        }\n\n    def get_stats(self):\n        return {\n            \"avg_memory_mb\": np.mean(self.memory_history),\n            \"avg_latency_s\": np.mean(self.latency_history),\n            \"total_requests\": len(self.memory_history)\n        }\n\n# Usage\nmonitor = QuantizationMonitor()\nclient = Inferneo(\"http://localhost:8000\")\n\n# Monitor different models\nmodels = [\n    \"meta-llama/Llama-2-7b-chat-hf\",\n    \"meta-llama/Llama-2-7b-chat-hf-awq\",\n    \"meta-llama/Llama-2-7b-chat-hf-gptq\"\n]\n\nfor model in models:\n    metrics = monitor.monitor_performance(\n        client, \n        \"Test prompt\", \n        model\n    )\n    print(f\"{model}: {metrics}\")\n\nprint(f\"Overall stats: {monitor.get_stats()}\")\n</code></pre>"},{"location":"user-guide/quantization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/quantization/#common-issues","title":"Common Issues","text":"<p>Out of Memory Errors <pre><code># Reduce model size or use more aggressive quantization\nclient.set_config(\n    gpu_memory_utilization=0.6,  # Reduce GPU memory usage\n    max_model_len=1024  # Reduce context length\n)\n</code></pre></p> <p>Slow Performance <pre><code># Use faster quantization method\nclient.set_quantization_config(\n    method=\"awq\",  # Generally faster than GPTQ\n    bits=4,\n    group_size=128\n)\n</code></pre></p> <p>Accuracy Degradation <pre><code># Use higher precision quantization\nclient.set_quantization_config(\n    method=\"gptq\",  # Better accuracy preservation\n    bits=4,\n    group_size=128,\n    desc_act=True\n)\n</code></pre></p> <p>For more advanced optimization techniques, see the Performance Tuning guide. </p>"},{"location":"user-guide/quickstart/","title":"Quickstart","text":""},{"location":"user-guide/quickstart/#overview","title":"Overview","text":"<p>Quick start guide for using Inferneo.</p>"},{"location":"user-guide/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/quickstart/#start-the-server","title":"Start the server","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf\n</code></pre>"},{"location":"user-guide/quickstart/#make-a-request","title":"Make a request","text":"<pre><code>import requests\n\nresponse = requests.post(\"http://localhost:8000/v1/chat/completions\", \n    json={\n        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }\n)\nprint(response.json())\n</code></pre>"},{"location":"user-guide/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Batching</li> <li>Streaming</li> <li>Distributed Inference </li> </ul>"},{"location":"user-guide/streaming/","title":"Streaming","text":""},{"location":"user-guide/streaming/#overview","title":"Overview","text":"<p>Guide for implementing real-time response streaming.</p>"},{"location":"user-guide/streaming/#basic-streaming","title":"Basic Streaming","text":""},{"location":"user-guide/streaming/#enable-streaming","title":"Enable Streaming","text":"<pre><code>import requests\n\nresponse = requests.post(\"http://localhost:8000/v1/chat/completions\", \n    json={\n        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n        \"stream\": True\n    },\n    stream=True\n)\n\nfor line in response.iter_lines():\n    if line:\n        print(line.decode('utf-8'))\n</code></pre>"},{"location":"user-guide/streaming/#websocket-streaming","title":"WebSocket Streaming","text":"<pre><code>import websockets\nimport json\n\nasync def stream_response():\n    async with websockets.connect('ws://localhost:8000/ws') as websocket:\n        await websocket.send(json.dumps({\n            \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n            \"stream\": True\n        }))\n\n        async for message in websocket:\n            data = json.loads(message)\n            print(data['content'], end='')\n</code></pre>"},{"location":"user-guide/streaming/#configuration","title":"Configuration","text":""},{"location":"user-guide/streaming/#stream-settings","title":"Stream Settings","text":"<pre><code>inferneo serve --model meta-llama/Llama-2-7b-chat-hf --enable-streaming\n</code></pre>"},{"location":"user-guide/streaming/#next-steps","title":"Next Steps","text":"<ul> <li>Batching</li> <li>Quantization</li> <li>Distributed Inference</li> <li>Performance Tuning guide </li> </ul>"}]}